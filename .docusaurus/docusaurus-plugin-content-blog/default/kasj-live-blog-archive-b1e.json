{
  "blogPosts": [
    {
      "id": "dns",
      "metadata": {
        "permalink": "/kasj-live/blog/dns",
        "source": "@site/blog/2023-02-23-dns/index.md",
        "title": "Adblocker and DNS server",
        "description": "I've always been meaning to add an adblocker to my home network and now with the additional need to have internal hostnames for my services this would be a great time to put one in. There were two great open source solutions to consider:",
        "date": "2023-02-23T00:00:00.000Z",
        "formattedDate": "February 23, 2023",
        "tags": [
          {
            "label": "adguard",
            "permalink": "/kasj-live/blog/tags/adguard"
          },
          {
            "label": "pihole",
            "permalink": "/kasj-live/blog/tags/pihole"
          },
          {
            "label": "dns",
            "permalink": "/kasj-live/blog/tags/dns"
          }
        ],
        "readingTime": 7.09,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "dns",
          "title": "Adblocker and DNS server",
          "authors": [
            "kas"
          ],
          "tags": [
            "adguard",
            "pihole",
            "dns"
          ]
        },
        "nextItem": {
          "title": "Persistent volumes and NFS",
          "permalink": "/kasj-live/blog/nfs"
        }
      },
      "content": "I've always been meaning to add an adblocker to my home network and now with the additional need to have internal hostnames for my services this would be a great time to put one in. There were two great open source solutions to consider:\n\n* **Pihole** - Pi-hole is a general purpose network-wide ad-blocker that protects your network from ads and trackers without requiring any setup on individual devices. It is able to block ads on any network device\n\n* **Adguard Home** - AdGuard Home is a network-wide software for blocking ads & tracking. After you set it up, it’ll cover ALL your home devices, and you don’t need any client-side software for that.\n\n\nHonestly I really can't tell the difference so I decided to install both to trial!\n\n## Pihole\n\n### Installing Pihole\n\nI wanted to create a bit of a file structure with all the required manifests which I can deploy at once:\n\n* `01-pihole-namespaces.yaml` - manifest to create a namespace\n* `02-pihole-configs.yaml` - manifest to specify configuration values such as whitelist domains and blocklists\n* `03-pihole-deployment.yaml` - manifest to specify deployment of pihole such as the container location\n* `04-pihole-service.yaml` - manifest to specify by port mappings and service exposure between container and pod\n\n:::note\nWorth noting here that I would normally be adding a `pihole-ingress.yaml` file here too to specify my traefik ingressRoute b resource but I won't be using traefik for this pihole or adguard home (as it will be a dns server)\n:::\n\nI also found out that you can run `kubectl apply -f` on and entire folder which deploys all the manifests within the folder specified so in my case:\n\n```bash\nkubectl apply -f pihole/\n```\n### Testing Pihole\n\nAs mentioned earlier, I didn't use Traefik for this service so I'm expecting that Metallb assigned a separate IP address allocated.\n\n![pihole](pihole.png)\n\nLooks good, so I just need to navigate to `http://192.168.86.101/admin` in my webbrowser to get to the admin portal\n\n![pihole2](pihole2.png)\n\nAn voila! Happy days. I can now use this as my DNS server, define some local DNS entries and start blocking some ads!\n\n## Adguard Home\n\n### Installing Adguard Home\n\nI thought I'd try installing Adguard Home slightly differently and use the Kompose tool instead. Kompose is simple, you give it a `docker-compose.yaml` and it outputs a set of kubernetes manifests for you.\n\nFirst things first, we need a docker-compose file so I head on over to docker hub to grab one for adguard. The docker-compose file looks like this:\n\n```yaml title=\"docker-compose.yaml\"\nversion: '3.3'\nservices:\n    adguard:\n        container_name: adguardhome\n        restart: unless-stopped\n        volumes:\n            - '/my/own/workdir:/opt/adguardhome/work'\n            - '/my/own/confdir:/opt/adguardhome/conf'\n        ports:\n            - '53:53/tcp'\n            - '53:53/udp'\n            - '67:67/udp'\n            - '68:68/udp'\n            - '80:80/tcp'\n            - '443:443/tcp'\n            - '443:443/udp'\n            - '3000:3000/tcp'\n            - '853:853/tcp'\n            - '784:784/udp'\n            - '853:853/udp'\n            - '8853:8853/udp'\n            - '5443:5443/tcp'\n            - '5443:5443/udp'\n        image: run\n```\n\nAfter installing [kompose](https://kompose.io/), all I run `kompose convert` to give my manifest files. Kompose gives me the following manifest files:\n\n* `adguard-claim0-persistentvolumeclaim.yaml`\n* `adguard-claim1-persistentvolumeclaim.yaml`\n* `adguard-deployment.yaml`\n* `adguard-service.yaml`\n\nTo see the manifest in detail, I've included them in the Appendix below. \n\n:::note\nI did need to make a slight change to the `adguard-service.yaml` auto generated file and that was to add the `LoadBalancer` service type. This tells Kubernetes that I needed and external IP from Metallb\n:::\n\nFinally I create a namespace and run all manifests with:\n\n```bash\nkubectl create namespace adguard\nkubectl apply -f adguard/ -n adguard\n```\n\n## Testing Adguard Home\n\nAs with PiHole, I was expecting to see the pods running and an external IP that I could navigate to with my browser:\n\n![adguard](adguard.png)\n\nSweet - looks like `192.168.86.102` was allocated.\n\n![adguard2](adguard2.png)\n\n## Closing thoughts\n\nBoth Pihole and Adguard Home are very similar from a feature set perspective so I haven't really managed to separate them as yet. If I was being super picky I'd say that Pihole is *slightly* more customisable with blocklists and Adguard Home has a *slightly* better UI. I haven't decided if one it better than the other so I'll keep them both running for now and switch DNS Servers from time to time.\n\n## Appendix\n\n### Pihole Manifests\n\nFor those interested in the manifests here they are:\n\n#### Namespace\n\n```yaml title=\"01-pihole-namespaces.yaml\"\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: pihole\n```\n\n#### Configuration\n\n\n```yaml title=\"02-pihole-configs.yaml\"\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom.list\n  namespace: pihole\ndata:\n  custom.list: |\n    192.168.86.41 k3smaster\n    192.168.86.40 k3snode01\n    192.168.86.100 traefik.local.kasj.live\n    192.168.86.101 pihole.local.kasj.live\n    192.168.86.100 dash.local.kasj.live\n    192.168.86.100 grocy.local.kasj.live\n    192.168.86.100 kuma.local.kasj.live\n    192.168.86.100 cloud.local.kasj.live\n    192.168.86.100 portainer.local.kasj.live\n    192.168.86.100 argocd.local.kasj.live\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: adlists.list\n  namespace: pihole\ndata:\n  adlists.list: |\n    https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts\n    https://adaway.org/hosts.txt\n    https://v.firebog.net/hosts/AdguardDNS.txt\n    https://v.firebog.net/hosts/Admiral.txt\n    https://raw.githubusercontent.com/anudeepND/blacklist/master/adservers.txt\n    https://s3.amazonaws.com/lists.disconnect.me/simple_ad.txt\n    https://v.firebog.net/hosts/Easylist.txt\n    https://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&showintro=0&mimetype=plaintext\n    https://raw.githubusercontent.com/FadeMind/hosts.extras/master/UncheckyAds/hosts\n    https://raw.githubusercontent.com/bigdargon/hostsVN/master/hosts\n    https://v.firebog.net/hosts/static/w3kbl.txt\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: whitelist.txt\n  namespace: pihole\ndata:\n  whitelist.txt: |\n    ichnaea.netflix.com\n    nrdp.nccp.netflix.com\n    androidtvchannels-pa.googleapis.com\n    lcprd1.samsungcloudsolution.net\n```\n\n#### Deployment\n\n```yaml title=\"03-pihole-deployment.yaml\"\nas@lappa:~$ cat home-lab/prod-apps/pihole/03-pihole-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: pihole\n  name: pihole\n  namespace: pihole\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pihole\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: pihole\n    spec:\n      containers:\n      - env:\n        - name: TZ\n          value: Australia/Melbourne\n        - name: WEBPASSWORD\n          value:\n        - name: DNS1\n          value: 9.9.9.9\n        - name: DNS2\n          value: 1.1.1.1\n        image: pihole/pihole:latest\n        imagePullPolicy: IfNotPresent\n        name: pihole\n        ports:\n        - name: dns-tcp\n          containerPort: 53\n          protocol: TCP\n        - name: dns-udp\n          containerPort: 53\n          protocol: UDP\n        - name: dhcp\n          containerPort: 67\n          protocol: UDP\n        - name: web\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        resources:\n          requests:\n            cpu: \"20m\"\n            memory: \"512Mi\"\n          limits:\n            cpu: \"250m\"\n            memory: \"896Mi\"\n        readinessProbe:\n          exec:\n            command: ['dig', '@127.0.0.1', 'cnn.com']\n          timeoutSeconds: 20\n          initialDelaySeconds: 5\n          periodSeconds: 60\n        livenessProbe:\n          tcpSocket:\n            port: dns-tcp\n          initialDelaySeconds: 15\n          periodSeconds: 30\n        volumeMounts:\n        - name: etc-pihole\n          mountPath: /etc/pihole\n        - name: etc-dnsmasq\n          mountPath: /etc/dnsmasq.d\n        - name: var-log\n          mountPath: /var/log\n        - name: var-log-lighttpd\n          mountPath: /var/log/lighttpd\n        - name: adlists\n          mountPath: /etc/pihole/adlists.list\n          subPath: adlists.list\n        - name: customlist\n          mountPath: /etc/pihole/custom.list\n          subPath: custom.list\n      restartPolicy: Always\n      volumes:\n      - name: etc-pihole\n        emptyDir:\n          medium: Memory\n      - name: etc-dnsmasq\n        emptyDir:\n          medium: Memory\n      - name: var-log\n        emptyDir:\n          medium: Memory\n      - name: var-log-lighttpd\n        emptyDir:\n          medium: Memory\n      - name: adlists\n        configMap:\n          name: adlists.list\n          items:\n            - key: adlists.list\n              path: adlists.list\n      - name: customlist\n        configMap:\n          name: custom.list\n          items:\n            - key: custom.list\n              path: custom.list\n```\n\n#### Service\n\n```yaml title=\"04-pihole-service.yaml\"\nkind: Service\napiVersion: v1\nmetadata:\n  name: pihole-udp\n  namespace: pihole\n  annotations:\n    metallb.universe.tf/allow-shared-ip: dns\nspec:\n  selector:\n    app: pihole\n  ports:\n  - protocol: UDP\n    port: 53\n    name: dnsudp\n    targetPort: 53\n  type: LoadBalancer\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: pihole-tcp\n  namespace: pihole\n  annotations:\n    metallb.universe.tf/allow-shared-ip: dns\nspec:\n  selector:\n    app: pihole\n  ports:\n  - protocol: TCP\n    port: 53\n    name: dnstcp\n    targetPort: 53\n  - protocol: TCP\n    port: 80\n    name: web\n    targetPort: 80\n  type: LoadBalancer\n```\n\n### Adguard Manifests\n\n#### Volume claims\n\n```yaml title=\"adguard-claim0-persistentvolumeclaim.yaml\"\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  creationTimestamp: null\n  labels:\n    io.kompose.service: adguard-claim0\n  name: adguard-claim0\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\nstatus: {}\n```\n\n```yaml title=\"adguard-claim1-persistentvolumeclaim.yaml\"\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  creationTimestamp: null\n  labels:\n    io.kompose.service: adguard-claim1\n  name: adguard-claim1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Mi\nstatus: {}\n```\n\n#### Deployment\n\n```yaml title=\"adguard-deployment.yaml\"\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.26.0 (40646f47)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: adguard\n  name: adguard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      io.kompose.service: adguard\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      annotations:\n        kompose.cmd: kompose convert\n        kompose.version: 1.26.0 (40646f47)\n      creationTimestamp: null\n      labels:\n        io.kompose.service: adguard\n    spec:\n      containers:\n        - image: adguard/adguardhome\n          name: adguardhome\n          ports:\n            - containerPort: 53\n            - containerPort: 53\n              protocol: UDP\n            - containerPort: 67\n              protocol: UDP\n            - containerPort: 68\n              protocol: UDP\n            - containerPort: 80\n            - containerPort: 443\n            - containerPort: 443\n              protocol: UDP\n            - containerPort: 3000\n            - containerPort: 853\n            - containerPort: 784\n              protocol: UDP\n            - containerPort: 853\n              protocol: UDP\n            - containerPort: 8853\n              protocol: UDP\n            - containerPort: 5443\n            - containerPort: 5443\n              protocol: UDP\n          resources: {}\n          volumeMounts:\n            - mountPath: /opt/adguardhome/work\n              name: adguard-claim0\n            - mountPath: /opt/adguardhome/conf\n              name: adguard-claim1\n      restartPolicy: Always\n      volumes:\n        - name: adguard-claim0\n          persistentVolumeClaim:\n            claimName: adguard-claim0\n        - name: adguard-claim1\n          persistentVolumeClaim:\n            claimName: adguard-claim1\nstatus: {}\n```\n\n#### Service\n\n```yaml title=\"adguard-service.yaml\"\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    kompose.cmd: kompose convert\n    kompose.version: 1.26.0 (40646f47)\n  creationTimestamp: null\n  labels:\n    io.kompose.service: adguard\n  name: adguard\nspec:\n  ports:\n    - name: \"53\"\n      port: 53\n      targetPort: 53\n    - name: 53-udp\n      port: 53\n      protocol: UDP\n      targetPort: 53\n    - name: \"67\"\n      port: 67\n      protocol: UDP\n      targetPort: 67\n    - name: \"68\"\n      port: 68\n      protocol: UDP\n      targetPort: 68\n    - name: \"80\"\n      port: 80\n      targetPort: 80\n    - name: \"443\"\n      port: 443\n      targetPort: 443\n    - name: 443-udp\n      port: 443\n      protocol: UDP\n      targetPort: 443\n    - name: \"3000\"\n      port: 3000\n      targetPort: 3000\n    - name: \"853\"\n      port: 853\n      targetPort: 853\n    - name: \"784\"\n      port: 784\n      protocol: UDP\n      targetPort: 784\n    - name: 853-udp\n      port: 853\n      protocol: UDP\n      targetPort: 853\n    - name: \"8853\"\n      port: 8853\n      protocol: UDP\n      targetPort: 8853\n    - name: \"5443\"\n      port: 5443\n      targetPort: 5443\n    - name: 5443-udp\n      port: 5443\n      protocol: UDP\n      targetPort: 5443\n  type: LoadBalancer\n  selector:\n    io.kompose.service: adguard\n```"
    },
    {
      "id": "nfs",
      "metadata": {
        "permalink": "/kasj-live/blog/nfs",
        "source": "@site/blog/2023-02-20-nfs/index.md",
        "title": "Persistent volumes and NFS",
        "description": "Containers and pods are ephemeral, kubernetes provides a great advantage of being able to orchestrate the deployment, scaling, deletion of pods. But what about storage? If we use a pod's local filesystem for a given application and that pod is deleted, the application data disappears with it. To solve for this, we need to leverage kubernetes storage classes. Kubernetes supports a number of various storage classes ranging from public cloud storage offering to local file storage. Think the best option for me is a NFS",
        "date": "2023-02-20T00:00:00.000Z",
        "formattedDate": "February 20, 2023",
        "tags": [
          {
            "label": "nfs",
            "permalink": "/kasj-live/blog/tags/nfs"
          },
          {
            "label": "persistent-volumes",
            "permalink": "/kasj-live/blog/tags/persistent-volumes"
          }
        ],
        "readingTime": 2.925,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "nfs",
          "title": "Persistent volumes and NFS",
          "authors": [
            "kas"
          ],
          "tags": [
            "nfs",
            "persistent-volumes"
          ]
        },
        "prevItem": {
          "title": "Adblocker and DNS server",
          "permalink": "/kasj-live/blog/dns"
        },
        "nextItem": {
          "title": "Certificates for HTTPS",
          "permalink": "/kasj-live/blog/certs"
        }
      },
      "content": "Containers and pods are ephemeral, kubernetes provides a great advantage of being able to orchestrate the deployment, scaling, deletion of pods. But what about storage? If we use a pod's local filesystem for a given application and that pod is deleted, the application data disappears with it. To solve for this, we need to leverage kubernetes **storage classes**. Kubernetes supports a number of [various storage classes](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner) ranging from public cloud storage offering to local file storage. Think the best option for me is a [NFS](https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs)\n\n## Installing an NFS server\n\nSo if I were to do this properly, I'd be running a NAS or NFS box but since I've skimped on the hardware, I'll be installing a NFS server on the same server as my cluster. You might be thinking *\"mate, that's just the same as local storage\"* and you would be right but I wanted to eventually switch to a separate NAS so figured I'd just learn how to do this.\n\nThere are plenty of tutorials available on how to install NFS on Ubuntu but i followed this one. Here are the key commands I took away to get the job done:\n\nInstall the NFS server and export `/nfs` which is accessible by the Kubernetes cluster:\n\n```bash\nsudo su\napt update && apt -y upgrade\napt install -y nfs-server\nexit\n\nmkdir /nfs\ncat << EOF >> /etc/exports\n/nfs 192.168.86.41(rw,no_subtree_check,no_root_squash)\nEOF\n\nsystemctl enable --now nfs-server\nexportfs -ar\n```\n\nIf I ever add another node to my cluster I need to ensure that a NFS client package is installed  able to connect to the NFS server but this isn't required as my NFS server is the same as my Kubernetes node:\n\n```bash\napt install -y nfs-common\n```\n\n## Persistent Volumes\n\nNow that I have a storage location, it is probably worth mentioning that the kubernetes rescource associated to persistent storage is **Persistent Volumes**. Like any other resource, I can provision persistent volumes declaritively to whatever storage class I specify.\n\nOnce a persistent volume is created, an application deployment can leverage the persistent volume using a **Persistent Volume Claim**. I could be wrong here but I think only one persistent volume claim can be applied to a persistent volume.\n\n## Dynamic Provisioning of Persistent Volumes\n\nKubernetes also provides you the ability to dynamically provision storage to applications. I found a nifty little tool that someone made called [NFS subdir external provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner) which is an automatic provisioner that uses your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as `${namespace}-${pvcName}-${pvName}`. To install this I run:\n\n```bash\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner\n\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  --create-namespace \\\n  --namespace nfs-provisioner \\\n  --set nfs.server=192.168.86.41 \\\n  --set nfs.path=/nfs\n```\n\n## Testing the provisioner\n\nTo test the provisioner I run:\n\n```bash\nkubectl get sc\n```\n\n![nfs](nfs.png)\n\nAnd will use following persistent volume claim manifest:\n\n```yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nfs-test\n  labels:\n    storage.k8s.io/name: nfs\n    storage.k8s.io/part-of: kubernetes-complete-reference\n    storage.k8s.io/created-by: ssbostan\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: nfs-client\n  resources:\n    requests:\n      storage: 1Gi\n```\n\nThings of note here:\n\n* `name` will vary for each volume claim - I will use the convention of `<app_name>-pvc`\n* `labels` doesn't change for my needs\n* `accessModes` doesn't change for my needs\n* `storageClassName` doesn't change for my needs\n* `storage` will vary for the app but worth noting that the whole specified range is provisioned (not just what you use)\n\nThat covers all the core cluster services I reckon, time to install some apps!"
    },
    {
      "id": "certs",
      "metadata": {
        "permalink": "/kasj-live/blog/certs",
        "source": "@site/blog/2023-02-16-letsencrypt/index.md",
        "title": "Certificates for HTTPS",
        "description": "Automatic Certificate Management Environment (ACME) Certificates can are usually provided through issuers. LetsEncrypt is a nonprofit Certificate Authority that provides free TLS certificates to millions of websites all around the world. This is was good enough for me!",
        "date": "2023-02-16T00:00:00.000Z",
        "formattedDate": "February 16, 2023",
        "tags": [
          {
            "label": "certificates",
            "permalink": "/kasj-live/blog/tags/certificates"
          },
          {
            "label": "letsencrypt",
            "permalink": "/kasj-live/blog/tags/letsencrypt"
          }
        ],
        "readingTime": 3.575,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "certs",
          "title": "Certificates for HTTPS",
          "authors": [
            "kas"
          ],
          "tags": [
            "certificates",
            "letsencrypt"
          ]
        },
        "prevItem": {
          "title": "Persistent volumes and NFS",
          "permalink": "/kasj-live/blog/nfs"
        },
        "nextItem": {
          "title": "Cluster reverse proxy",
          "permalink": "/kasj-live/blog/traefik"
        }
      },
      "content": "Automatic Certificate Management Environment (ACME) Certificates can are usually provided through issuers. LetsEncrypt is a nonprofit Certificate Authority that provides free TLS certificates to millions of websites all around the world. This is was good enough for me!\n\n## Adding cloudflare token to cert-manager\n\nFirst I needed a domain name which I purchased through CloudFlare but can be from anywhere really. You guessed it - mine is `kasj.live`. From there I needed to obtain an cloudflare token which was a personal access token to manage my DNS records in my cloudflare account. I needed this as I needed to provide it to cert-manager, which will be brokering the certificates between letsencrypt and my domain.\n\nProviding cert-manager my cloudflare token could be done with a simple manifest:\n\n```yaml title=secret-cf-token.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cloudflare-token-secret\n  namespace: cert-manager\ntype: Opaque\nstringData:\n  cloudflare-token: <redacted>\n```\n\nTo apply the manifest run:\n\n```bash\nkubectl apply -f secret-cf-token.yaml\n```\n\n## Adding Let's Encrypt as an Issuer to cert-manager\n\nI now need to let cert-manager know that I'll be using Let's Encrypt as my certificate issuer of choice through another manifest:\n\n```yaml title=\"letsencrypt-production.yaml\"\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-production\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: kasunj@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt-production\n    solvers:\n      - dns01:\n          cloudflare:\n            email: kasunj@gmail.com\n            apiTokenSecretRef:\n              name: cloudflare-token-secret\n              key: cloudflare-token\n        selector:\n          dnsZones:\n            - \"kasj.live\"\n```\n\nand execute using:\n\n```bash\nkubectl apply -f letsencrypt-production.yaml\n```\n\n## Issuing certificates\n\nWith the issuer now configured, all I need to do is request for a certificate. I will be hosting all my internal applications under the subdomain `local.kasj.live` so i will request for a wildcard certicate that covers `*.local.kasj.live`\n\nThe certificate is issued with the following manifest:\n\n```yaml title=\"local-kasj-live.yaml\"\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: local-kasj-live\n  namespace: default\nspec:\n  secretName: local-kasj-live-tls\n  issuerRef:\n    name: letsencrypt-production\n    kind: ClusterIssuer\n  commonName: \"*.local.kasj.live\"\n  dnsNames:\n  - \"local.kasj.live\"\n  - \"*.local.kasj.live\"\n```\n\nand execute using:\n\n```bash\nkubectl apply -f local-kasj-live.yaml\n```\n\nIssuing  and validating the certificates takes time (20 minutes minimum). To check how things are progressing run:\n\n```bash\nkubectl get challenges\n```\n\n:::caution\n\nYou'll notice that I use the issuer name `letsencrypt-production` - I didn't jump straight to this but rather used `letsencrypt-staging` first to make sure all my configuration was correct. If you jump straight to production but if it doesn't work for whatever reason you might be locked out by letsencrypt for a period of time.\n\n:::\n\n## Testing the issued certificate\n\nOnce the `kubectl get challenges` command produces nothing, that's when you know the process is complete. To use a certificate, you need to ensure a couple of things:\n\n* The certificate needs to be made available in multiple namespaces. The certificate only works if it is deployed in the same namespaces as the service you are using it for. With a bit of googling I've been using the following [solution](https://github.com/mittwald/kubernetes-replicator) for this.\n\n* We use Traefik to specify and `ingressRoute` which essentionally provides traefik with the instructions on where to route traffic hitting the reverse proxy. We can also specify here that a certificate must be used.\n\nTo test above, I deployed the Traefik dashboard (with the help of their documentation and TechnoTim) with the following steps:\n\nCreate and deploy a middleware manifest that forces https:\n\n```yaml title=\"middleware.yaml\"\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: traefik-dashboard-basicauth\n  namespace: traefik\nspec:\n  basicAuth:\n    secret: traefik-dashboard-auth\n```\n\nGenerate a credential whichi is mandatory for the dashboard:\n\n```bash\n# Generate a credential / password that’s base64 encoded\nhtpasswd -nb kas <redacted> | openssl base64\n```\n\nCreate and apply a manifest to deploy the dashboard. Note you need to use the output from command above for the password:\n\n```yaml\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: traefik-dashboard-auth\n  namespace: traefik\ntype: Opaque\ndata:\n  users: <redacted hased password which is output from above>\n```\n\nFinally I create a manifest for an `ingressRoute` which will route traffic from `traefik.local.kasj.live` to my dashboard using TLS certificate I just created:\n\n```yaml title=\"traefik-ingress.yaml\"\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-dashboard\n  namespace: traefik\n  annotations:\n    kubernetes.io/ingress.class: traefik-external\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`traefik.local.kasj.live`)\n      kind: Rule\n      middlewares:\n        - name: traefik-dashboard-basicauth\n          namespace: traefik\n      services:\n        - name: api@internal\n          kind: TraefikService\n  tls:\n    secretName: local-kasj-live-tls\n```\n\n## And the results\n\nSo now if I navigate to `https://traefik.local.kasj.live` I can not see the traefik dashboard\n\n![traefik](traefik.png)\n\nAnd more importantly with a certificate issued from Let's Encrypt!\n\n![cert](cert.png)"
    },
    {
      "id": "traefik",
      "metadata": {
        "permalink": "/kasj-live/blog/traefik",
        "source": "@site/blog/2023-02-13-traefik/index.md",
        "title": "Cluster reverse proxy",
        "description": "Now that I have load balancer to expose my services externally, I have a couple of options:",
        "date": "2023-02-13T00:00:00.000Z",
        "formattedDate": "February 13, 2023",
        "tags": [
          {
            "label": "reverse-proxy",
            "permalink": "/kasj-live/blog/tags/reverse-proxy"
          },
          {
            "label": "traefik",
            "permalink": "/kasj-live/blog/tags/traefik"
          },
          {
            "label": "kubernetes",
            "permalink": "/kasj-live/blog/tags/kubernetes"
          }
        ],
        "readingTime": 2.065,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "traefik",
          "title": "Cluster reverse proxy",
          "authors": [
            "kas"
          ],
          "tags": [
            "reverse-proxy",
            "traefik",
            "kubernetes"
          ]
        },
        "prevItem": {
          "title": "Certificates for HTTPS",
          "permalink": "/kasj-live/blog/certs"
        },
        "nextItem": {
          "title": "Cluster load balancer",
          "permalink": "/kasj-live/blog/metallb"
        }
      },
      "content": "Now that I have load balancer to expose my services externally, I have a couple of options:\n\n* Expose every service I deploy over metallb (ie. each app gets its own IP address) or;\n* Deploy a **reverse proxy** which intercepts and routes every incoming request to the corresponding backend services.\n\nFrom the title, you can tell which option I went with. I went with the reverse proxy option because\n\n* I don't know how many applications I will eventually host\n* I also don't need to think about which application is associated with which IP and configure DNS routes etc\n* It can also provide SSL termination and can be used with an ACME provider (like Let’s Encrypt) for automatic certificate generation (which I'll cover in a future post)\n\n## Installing Traefik\n\nLike Metallb, there are heaps of reverse proxy options out there but I went with a popular option [Traefik](https://traefik.io/traefik/).\n\n![traefik](https://traefik.io/static/83ea42c9e8101dcf2a16f380fe3aac08/053ba/diagram.webp)\n\nI wanted to try installing this via helm this time. Helm allows you specify custom configuration values via a `values.yaml` file so I did that first. I know its quite long but I just tweaked the defaults:\n\n```yaml title=\"values.yaml\"\nglobalArguments:\n  - \"--global.sendanonymoususage=false\"\n  - \"--global.checknewversion=false\"\n\nadditionalArguments:\n  - \"--serversTransport.insecureSkipVerify=true\"\n  - \"--log.level=INFO\"\n\ndeployment:\n  enabled: true\n  replicas: 1\n  annotations: {}\n  podAnnotations: {}\n  additionalContainers: []\n  initContainers: []\n\nports:\n  web:\n    redirectTo: websecure\n  websecure:\n    tls:\n      enabled: true\n\ningressRoute:\n  dashboard:\n    enabled: false\n\nproviders:\n  kubernetesCRD:\n    enabled: true\n    ingressClass: traefik-external\n    allowExternalNameServices: true\n  kubernetesIngress:\n    enabled: true\n    allowExternalNameServices: true\n    publishedService:\n      enabled: false\n\nrbac:\n  enabled: true\n\nservice:\n  enabled: true\n  type: LoadBalancer\n  annotations: {}\n  labels: {}\n  spec:\n    loadBalancerIP: 192.168.86.100 # this should be an IP in the MetalLB range\n  loadBalancerSourceRanges: []\n  externalIPs: []\n```\n\nThen I needed to execute these commands to install via helm (after installing [helm](https://helm.sh/) of course):\n\nAdd repo\n```bash\nhelm repo add traefik https://helm.traefik.io/traefik\n```\n\nUpdate repo\n```bash \nhelm repo update\n```\nCreate namespace\n```bash\nkubectl create namespace traefik\n```\nFinally install using helm and our custom values file:\n```bash\nhelm install --namespace=traefik traefik traefik/traefik --values=values.yaml\n```\n\n## Verifying installation\n\nFinally it was time to check if installation succeeded. \n\n![traefikverify](traefikverify.png)\n\nWhat a beautiful sight, it is all working. The main thing I was happy to see was that Metallb did the job too by assigning the IP `192.168.86.100` to the traefik service. This means I can now route all incoming request (regardless of which application) to this IP and traefik will handle all the routing. This will be done through domain names which will be covered in a later post."
    },
    {
      "id": "metallb",
      "metadata": {
        "permalink": "/kasj-live/blog/metallb",
        "source": "@site/blog/2023-02-10-metallb/index.md",
        "title": "Cluster load balancer",
        "description": "So now I have a cluster - woot! My first thing was to get stuck in and deploying some apps but quickly realised there were a couple of other things that needed to be considered. Say I deployed a web server to my cluster using a small nginx container. It's up and running but how do I access it when it is only routable within my cluster. if you look at the pods IP addresses, they are all 10.1.x.x which is not in my home network.",
        "date": "2023-02-10T00:00:00.000Z",
        "formattedDate": "February 10, 2023",
        "tags": [
          {
            "label": "loadbalancer",
            "permalink": "/kasj-live/blog/tags/loadbalancer"
          },
          {
            "label": "metallb",
            "permalink": "/kasj-live/blog/tags/metallb"
          },
          {
            "label": "kubernetes",
            "permalink": "/kasj-live/blog/tags/kubernetes"
          }
        ],
        "readingTime": 2.99,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "metallb",
          "title": "Cluster load balancer",
          "authors": [
            "kas"
          ],
          "tags": [
            "loadbalancer",
            "metallb",
            "kubernetes"
          ]
        },
        "prevItem": {
          "title": "Cluster reverse proxy",
          "permalink": "/kasj-live/blog/traefik"
        },
        "nextItem": {
          "title": "Kubernetes cluster - K3s",
          "permalink": "/kasj-live/blog/k3s"
        }
      },
      "content": "So now I have a cluster - woot! My first thing was to get stuck in and deploying some apps but quickly realised there were a couple of other things that needed to be considered. Say I deployed a web server to my cluster using a small nginx container. It's up and running but how do I access it when it is only routable within my cluster. if you look at the pods IP addresses, they are all *10.1.x.x* which is not in my home network.\n\nWell, the answer is I need use a load balancer to expose that web server outside the cluster so that I can see it on my home network. When working in public cloud, these load balancers are usually provided by the cloud providers but I need one for my locally hosted environment.\n\nAfter a bit of research, I found that [metallb](https://metallb.universe.tf/) is the best solution for this. You give it a \"pool\" of IP addresses within your home network to allocate to services that you want to expose and it just does it (much like a DHCP server)\n\n## Installing applications\n\nSo this is my first app that I am going to install on my cluster so it took me a little bit of reading to get to this point but here are my key takeaways of installing this and any app:\n\n* You can specify kubernetes a *manifest* which is basically a yaml file which allows you to declaritively specify what you want to install, where to install it from and what configurations you want and how to expose it.\n* Most container applications are containerised with docker and often come with an associated docker-compose file. There is a nifty tool called [kompose](https://kompose.io/) which allows you to take these docker-compose files and it converts it to a kubernetes manifest for you to allow it to be deployed to your cluster - I plan on using this a lot.\n* Another popular way of installing application is Helm. Helm is a package manager similar to apt (if you're familiar with Ubuntu) which allows you to easily install applications on to your cluster. All you need to do is specify the repo for the application you are wanting to install and it does the rest - I plan on using this a lot too.\n\n## Installing metallb\n\nInstalling metallb is pretty easy out of the box. You are already provided with manifest to deploy using the **kubectl apply** command:\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.9/config/manifests/metallb-native.yaml\n```\n\nThis installs metallb into a new **namespace** called **metallb-system**. Namespaces are a kubenetes construct that basically allow you a way to organise resources within your cluster. I like to think of them as \"folders\" in a typical file system. So for metallb, all my resources will live in the metallb-system namespace. This allows for easy troubleshooting in the future as I know where they all live.\n\n\n## Configuring metallb\n\nOnce installed, there were some configuration changes that needed to be made. As mentioned earlier, I needed to specify a pool of IP address for metallb to allocate out. I put this into another yaml file:\n\n```yaml title=\"/home-lab/cluster-setup/metallb/metallb-ipconfig.yaml\"\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: first-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.86.100-192.168.86.110\n\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: default\n  namespace: metallb-system\n```\n\n## Verifying installation\n\nAll done! Here are a few commands to verify my install:\n\n![metallbverify](metallbverify.png)\n\nBut the real test is if it will allocate an IP to a service. Let's test it with reverse proxy service. Stay tuned as I will cover this in my next post!"
    },
    {
      "id": "k3s",
      "metadata": {
        "permalink": "/kasj-live/blog/k3s",
        "source": "@site/blog/2023-01-25-k3s/index.md",
        "title": "Kubernetes cluster - K3s",
        "description": "So with my hardware set up, it was time to get my software up and running. One of the main objectives of setting up this homelab was to get familiar with kubernetes so we need to get a cluster up and running so I can do more than this:",
        "date": "2023-01-25T00:00:00.000Z",
        "formattedDate": "January 25, 2023",
        "tags": [
          {
            "label": "k8s",
            "permalink": "/kasj-live/blog/tags/k-8-s"
          },
          {
            "label": "k3s",
            "permalink": "/kasj-live/blog/tags/k-3-s"
          },
          {
            "label": "kubernetes",
            "permalink": "/kasj-live/blog/tags/kubernetes"
          }
        ],
        "readingTime": 2.695,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "k3s",
          "title": "Kubernetes cluster - K3s",
          "authors": [
            "kas"
          ],
          "tags": [
            "k8s",
            "k3s",
            "kubernetes"
          ]
        },
        "prevItem": {
          "title": "Cluster load balancer",
          "permalink": "/kasj-live/blog/metallb"
        },
        "nextItem": {
          "title": "Clean up",
          "permalink": "/kasj-live/blog/cable-management"
        }
      },
      "content": "So with my hardware set up, it was time to get my software up and running. One of the main objectives of setting up this homelab was to get familiar with kubernetes so we need to get a cluster up and running so I can do more than this:\n\n![dilbert](https://pbs.twimg.com/media/EDrZEKCWwAAG_Ty.jpg)\n\n## Installing an OS\n\nI needed to get an OS installed on my NUC before anything else. There are plenty of open source options out there but I stuck with trusty [Ubuntu Server 22.04.1 LTS ](https://ubuntu.com/download/server)\n\n## K8s vs K3s\n\nBefore I got stuck into deploying my kubernetes I wanted to investigate what options I had for a homelab. It boiled down to two main ones: K8s vs K3s. Both K8s and K3s share the same source code but the key difference for me was that K3s was significantly more lightweight, can be deployed much faster and still all all the key capabilities of K8s. There are a number of \"production grade\" features that are excluded from K3s such has handling of complex applications and intergrations with public cloud providers which I didn't require.\n\n## Installing K3s\n\nInstalling K3s couldn't be more easier out of the box and it takes no time at all. I simply needed to ssh into my freshly install ubuntu server and execute the following command:\n\n```bash\ncurl -sfL https://get.k3s.io | sh - \n```\n\nand to uninstall it is:\n\n```bash\n/usr/local/bin/k3s-uninstall.sh\n```\n\nI must admit I ran these commands **ALOT** because there were a number of things that were installed by default which I didn't need (yet). I'm not going to get into the various customisable configuration options here but there is some pretty good [documentation](https://docs.k3s.io/installation/configuration) for it. After tweaking my configuration, I ended up with the following command to install the cluster I wanted:\n\n```bash\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" INSTALL_K3S_EXEC=\"--disable traefik --disable servicelb --disable kube-proxy --disable local-storage --cluster-init --tls-san 10.43.0.1\" sh -s -\n```\n\n## Accessing my K3s cluster\n\nOk time to run my first kubectl command. To verify that everything was running properly I run:\n\n```bash\nsudo kubectl get nodes\n```\n\nwhich shows me my single node in my cluster is up and running:\n\n![getnodes](./getnodes.png)\n\nI needed to run it in sudo which I thought was annoying -I had to fix this (OCD much?). The K3s kubeconfig file is stored at a rancher location /etc/rancher/k3s. \nI *think* this is why I needed to run kubectl in sudo. So I ran the following steps to rectify that:\n\nCreate .kube directory in my home directory\n\n```bash\nsudo mkdir /home/kas/.kube\n```\n\nCopy the kubeconfig file into the newly created directory\n\n```bash\nsudo cp /etc/rancher/k3s/k3s.yaml /home/kas/.kube/config\n```\n\nChange ownership of the directory so that root wasn't needed\n```bash\nsudo chown kas:kas /home/kas/.kube/config\n```\nLet K3s know the location of the new config file (and hopefully the last time I have to use sudo for kubectl)\n\n```bash\nsudo kubectl config set-cluster default --server=https://192.168.86.41:6443 --kubeconfig /home/kas/.kube/config\n```\n\nI wanted to ensure I could access my cluster from my laptop without having to SSH into my ubuntu server (k3smaster) everytime. To do this I needed copy the kubeconfig file across to my laptop using scp:\n\nFrom my laptop I run:\n\n```bash\nscp k3smaster:/home/kas/.kube/config /home/kas/.kube/config\n```\n\n...and I'm laughing::\n\n![getnodes](./getnodes-lappa.png)"
    },
    {
      "id": "cable-management",
      "metadata": {
        "permalink": "/kasj-live/blog/cable-management",
        "source": "@site/blog/2023-01-20-cables/index.md",
        "title": "Clean up",
        "description": "So first things first, just like a chef needs to keep their bench clean, think this is the perfect time to clean up my office table. More importantly this cable situation that has been driving me INSANE",
        "date": "2023-01-20T00:00:00.000Z",
        "formattedDate": "January 20, 2023",
        "tags": [
          {
            "label": "cable-management",
            "permalink": "/kasj-live/blog/tags/cable-management"
          },
          {
            "label": "desk",
            "permalink": "/kasj-live/blog/tags/desk"
          }
        ],
        "readingTime": 0.565,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "cable-management",
          "title": "Clean up",
          "authors": [
            "kas"
          ],
          "tags": [
            "cable-management",
            "desk"
          ]
        },
        "prevItem": {
          "title": "Kubernetes cluster - K3s",
          "permalink": "/kasj-live/blog/k3s"
        },
        "nextItem": {
          "title": "Gear up",
          "permalink": "/kasj-live/blog/hardware"
        }
      },
      "content": "So first things first, just like a chef needs to keep their bench clean, think this is the perfect time to clean up my office table. More importantly this cable situation that has been driving me **INSANE**\n\n![Untidy Cables](./untidy_cables.jpg)\n\n### Cable Management\n\nI've had a couple of attempts at cable management but it has always been *half-arsed* and heavy on the duct tape. It was time to do this properly. I bought a couple of [cable cradles](https://www.amazon.com.au/gp/product/B09NVYV5NB/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1) from Amazon that fit nicely under the table. From there it was relativey easy to run all the cables and extension cords through. Here's the final result:\n\n![Tidy Cables 1](./tidycable_1.jpg)\n![Tidy Cables 2](./tidycable_2.jpg)\n![Tidy Cables 3](./tidycable_3.jpg)"
    },
    {
      "id": "hardware",
      "metadata": {
        "permalink": "/kasj-live/blog/hardware",
        "source": "@site/blog/2023-01-17-hardware/index.md",
        "title": "Gear up",
        "description": "I've normally resorted to buying raspberry pi's (I now have a Model 2B, 3B and 4) and don't have any major self-hosting requirements so I considered clustering them together. Given the current ARM architecture limitations on the older models, I decided I will bite the bullet and buy some more dedicated hardware. I also figured I could:",
        "date": "2023-01-17T00:00:00.000Z",
        "formattedDate": "January 17, 2023",
        "tags": [
          {
            "label": "homelab",
            "permalink": "/kasj-live/blog/tags/homelab"
          },
          {
            "label": "self-hosting",
            "permalink": "/kasj-live/blog/tags/self-hosting"
          },
          {
            "label": "nuc",
            "permalink": "/kasj-live/blog/tags/nuc"
          },
          {
            "label": "hardware",
            "permalink": "/kasj-live/blog/tags/hardware"
          }
        ],
        "readingTime": 1.1,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "hardware",
          "title": "Gear up",
          "authors": [
            "kas"
          ],
          "tags": [
            "homelab",
            "self-hosting",
            "nuc",
            "hardware"
          ]
        },
        "prevItem": {
          "title": "Clean up",
          "permalink": "/kasj-live/blog/cable-management"
        },
        "nextItem": {
          "title": "Welcome",
          "permalink": "/kasj-live/blog/welcome"
        }
      },
      "content": "I've normally resorted to buying raspberry pi's (I now have a Model 2B, 3B and 4) and don't have any major self-hosting requirements so I considered clustering them together. Given the current ARM architecture limitations on the older models, I decided I will bite the bullet and buy some more dedicated hardware. I also figured I could:\n\n* Add the raspberry pi's into the cluster for some more resources if need be at a later date. \n* Use the raspberry pi's as an isolated sandbox for testing\n\nWith Raspberry Pi 4's currently in low stocks, I started exploring mini PCs/NUCs. They are actually pretty cool, the specs are much better, the costs aren't that much higher and they are built to last.\n\nI ended up getting [Intel NUC 11 Essential Kit Celeron N4505 (Atlas Canyon)](https://www.centrecom.com.au/intel-nuc-11-essential-kit-celeron-n4505-atlas-canyon) which was on special at CentreCom. I unboxed and plugged it in immediately only to realise that it is pretty barebones unfortunately. \n\n![NUC](https://cdn1.centrecom.com.au/images/upload/0135473_0.jpeg)\n\nThe site doesn't cover it but it doesn't come with any pre-installed HDD or RAM (that's why it was cheap!) so I needed to get those off Amazon. Here's what I bought:\n\n2 X [8GB DDR4 RAM](https://www.amazon.com.au/dp/B08C4Z69LN?psc=1&ref=ppx_yo2ov_dt_b_product_details)\n  \n  ![RAM](https://m.media-amazon.com/images/I/71exOjbZWiL._AC_SX679_.jpg)\n\n1 x [500GB SSD Hard Drive](https://m.media-amazon.com/images/I/418VuyafoUL._AC_SL1075_.jpg)\n  \n  ![HDD](https://cdn.mwave.com.au/images/400/crucial_p5_plus_500gb_nvme_m2_pcie_3d_nand_ssd_ct500p5pssd8_ac46349.jpg)\n\n\nInstalling them were pretty easy - plug n play, I was then ready to rock n roll!"
    },
    {
      "id": "welcome",
      "metadata": {
        "permalink": "/kasj-live/blog/welcome",
        "source": "@site/blog/2023-01-15-welcome/index.md",
        "title": "Welcome",
        "description": "I've been dabbling with a few self hosted services on my laptop or a raspberry pi for a while now but I now have a need to run some of these services on going. I've also wanted to become more comfortable with kubernetes administration so this is also a great opportunity for me to learn something but also run a few practical services at home.",
        "date": "2023-01-15T00:00:00.000Z",
        "formattedDate": "January 15, 2023",
        "tags": [
          {
            "label": "homelab",
            "permalink": "/kasj-live/blog/tags/homelab"
          },
          {
            "label": "self-hosting",
            "permalink": "/kasj-live/blog/tags/self-hosting"
          }
        ],
        "readingTime": 0.975,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "welcome",
          "title": "Welcome",
          "authors": [
            "kas"
          ],
          "tags": [
            "homelab",
            "self-hosting"
          ]
        },
        "prevItem": {
          "title": "Gear up",
          "permalink": "/kasj-live/blog/hardware"
        },
        "nextItem": {
          "title": "Certificate manager for cluster",
          "permalink": "/kasj-live/blog/cert-manager"
        }
      },
      "content": "I've been dabbling with a few self hosted services on my laptop or a raspberry pi for a while now but I now have a need to run some of these services on going. I've also wanted to become more comfortable with kubernetes administration so this is also a great opportunity for me to learn something but also run a few practical services at home.\n\nThere are some amazing resources available that I have been leaning on heavily but I find writing things down will help make some of the key concepts stick. I'm hoping I can then transfer some of these skills to my work and future projects.\n\nI wanted to give a shout out to [technotim](https://www.youtube.com/@technotim) and [noted.lol](https://noted.lol/) who I've drawn a lot of inspiration from for a number of my services.\n\nFull disclosure, I'm not one to write blogs so this is a bit of a challenge in itself. I've given myself a bit of license to be a keep things a bit rough as I write to keep myself motivated to continue to post regularly.\n\nIf you're interested feel free to read through my posts and head over to my [docs](/docs/intro)"
    },
    {
      "id": "cert-manager",
      "metadata": {
        "permalink": "/kasj-live/blog/cert-manager",
        "source": "@site/blog/2023-01-15-certmanager/index.md",
        "title": "Certificate manager for cluster",
        "description": "Certificates in K3s",
        "date": "2023-01-15T00:00:00.000Z",
        "formattedDate": "January 15, 2023",
        "tags": [
          {
            "label": "cert-manager",
            "permalink": "/kasj-live/blog/tags/cert-manager"
          },
          {
            "label": "kubernetes",
            "permalink": "/kasj-live/blog/tags/kubernetes"
          }
        ],
        "readingTime": 0.855,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Kas J",
            "title": "Author",
            "url": "https://github.com/kasjayatissa",
            "imageURL": "https://avatars.githubusercontent.com/u/90017589?v=4",
            "key": "kas"
          }
        ],
        "frontMatter": {
          "slug": "cert-manager",
          "title": "Certificate manager for cluster",
          "authors": [
            "kas"
          ],
          "tags": [
            "cert-manager",
            "kubernetes"
          ]
        },
        "prevItem": {
          "title": "Welcome",
          "permalink": "/kasj-live/blog/welcome"
        }
      },
      "content": "## Certificates in K3s\n\nIn my previous post I mentioned that Traefik allows me to provide SSL termination certificate handling. The thing is certificates are actually an unknown resource type in the kubernetes ecosystem like \"pods\" or \"services\". \n\n[cert-manager](https://cert-manager.io/) adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\n\n![certmanager](https://cert-manager.io/images/high-level-overview.svg)\n\n## Installing cert-manager\n\nAdd customer resource definition (CRD) using a manifest from cert-manager:\n\n```bash\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\n```\n\nLike with traefik, I also created a `values.yaml` file for the helm installation:\n\n```yaml title=\"values.yaml\"\ninstallCRDs: false # Oops didn't realise I could do it here\nreplicaCount: 1\nextraArgs:\n  - --dns01-recursive-nameservers=1.1.1.1:53,9.9.9.9:53\n  - --dns01-recursive-nameservers-only\npodDnsPolicy: None\npodDnsConfig:\n  nameservers:\n    - \"1.1.1.1\"\n    - \"9.9.9.9\"\n```\nCreate namespace, add the repo and update the repo\n\n```bash\nkubectl create namespace cert-manager\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\n```\n\nInstall cert-manager via helm\n\n```bash\nhelm install cert-manager jetstack/cert-manager --namespace cert-manager --values=values.yaml --version v1.11.0\n```\n\nWith cert-manager now installed it was time get some certificates!"
    }
  ]
}