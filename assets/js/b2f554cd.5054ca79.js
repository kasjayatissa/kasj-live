"use strict";(self.webpackChunkkasj_live=self.webpackChunkkasj_live||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"ansible","metadata":{"permalink":"/blog/ansible","source":"@site/blog/2023-04-12-ansible/index.md","title":"Automating configurations with Ansible","description":"Over the past couple of weeks, I\'ve been experimenting quite a bit on my server (installing, uninstalling, changing config everywhere). Before I started installing and using my apps in anger I thought it might be best to start clean. This was also a great opporunity to learn Ansible to automate the entire cluster set up.","date":"2023-04-12T00:00:00.000Z","formattedDate":"April 12, 2023","tags":[{"label":"ansible","permalink":"/blog/tags/ansible"},{"label":"automation","permalink":"/blog/tags/automation"}],"readingTime":1.97,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"ansible","title":"Automating configurations with Ansible","authors":["kas"],"tags":["ansible","automation"]},"nextItem":{"title":"I switched to a self-hosted password manager and I\'m not going back","permalink":"/blog/bitwarden"}},"content":"Over the past couple of weeks, I\'ve been experimenting quite a bit on my server (installing, uninstalling, changing config everywhere). Before I started installing and using my apps in anger I thought it might be best to start clean. This was also a great opporunity to learn Ansible to automate the entire cluster set up.\\n\\nI\'ve been using Terraform quite a bit as part of my day job and for me Ansible is Terraform\'s perfect partner in crime. Terraform sets up the infrastructure and Ansible then goes over the top and installs the configuration. Both tools operate under the same way, configurations describe the state that you need in a declarative manner. So for cluster set up my target is to automate the following tasks after a fresh version of Ubuntu is installed:\\n\\n1. Install K3s\\n2. Install load balancer Metallb\\n3. Install NFS server\\n4. Install reverse proxy Traefik\\n5. Install Cert Manager and Certificates\\n6. Install ArgoCD (which will deploy all my apps - more on this in a future post)\\n\\n## Ansible playbooks and roles\\n\\nAnsible configurations are written in a file which contains all the various config tasks that are needed to be run along with instructions to execute these tasks called **ansible playbooks**. You could theoretically write one big playbook to automate all the tasks above but it is easier to keep things modular. This is where **ansible roles** comes in - roles are simply a more modular version of a playbook which contains its own file structure, variables and handlers. More importantly for me, grouping things by roles allow you to share your playbooks easier with other users. \\n\\nThrough the wonderful open source community I was able to source some great roles through Ansible galaxy (Ansible role marketplace) and Github, I was able to source and tweak the following roles:\\n\\n* Techno Tim\'s role to install [K3s and Metallb](https://github.com/techno-tim/k3s-ansible)\\n* Jeff Geerling\'s role to install [NFS server](https://galaxy.ansible.com/geerlingguy/nfs)\\n* Githubixx\'s role to install [cert-manager](https://galaxy.ansible.com/githubixx/cert_manager_kubernetes) and [traefik](https://galaxy.ansible.com/githubixx/traefik_kubernetes)\\n* Reefland\'s role to install [ArgoCD](https://github.com/reefland/ansible-k3s-argocd-renovate)\\n\\nHonestly, I didn\'t need to make many tweaks to these playbooks at all, if anything I needed to remove tasks because they were doing more than I needed to do. All I needed to do was point these playbooks to my local servers, ensure I had SSH connectivity and the playbooks did the rest - Pretty cool!"},{"id":"bitwarden","metadata":{"permalink":"/blog/bitwarden","source":"@site/blog/2023-03-22-passwords/index.md","title":"I switched to a self-hosted password manager and I\'m not going back","description":"One of the biggest reasons for creating a homelab is I wanted the ability to leverage the some of the great services and solutions that were available in the public cloud within the safety of my home network where my data was out of reach in the wild world of the internet. I can\'t think of a better example of this than Password Management.","date":"2023-03-22T00:00:00.000Z","formattedDate":"March 22, 2023","tags":[{"label":"passwords","permalink":"/blog/tags/passwords"},{"label":"bitwarden","permalink":"/blog/tags/bitwarden"}],"readingTime":2.945,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"bitwarden","title":"I switched to a self-hosted password manager and I\'m not going back","authors":["kas"],"tags":["passwords","bitwarden"]},"prevItem":{"title":"Automating configurations with Ansible","permalink":"/blog/ansible"},"nextItem":{"title":"Fixing my wildcard certificates","permalink":"/blog/certv2"}},"content":"One of the biggest reasons for creating a homelab is I wanted the ability to leverage the some of the great services and solutions that were available in the public cloud within the safety of my home network where my data was out of reach in the wild world of the internet. I can\'t think of a better example of this than Password Management. \\n\\nThe last 12 months have been incredibly eye opening for a lot of folks when it comes to data privacy with some pretty large breaches impacting millions of people. The Office of the Australian Information Commissioner [reports](https://www.oaic.gov.au/privacy/notifiable-data-breaches/notifiable-data-breaches-publications/notifiable-data-breaches-report-july-to-december-2022) 497 breaches were notified compared with 393 in January to June 2022 \u2013 a 26% increase, most of which were malicious or criminal attacks.\\n\\nSo many of us (me included) have so many accounts/subscriptions/emails, it is very hard for us to keep up with strong password requirements and practices. Luckily for us there are a number of excellent solutions to generate and manage passwords for us so we don\'t have to. Passwords can now be long and complicated which dramatically reduce the chances of being hacked or exploited. \\n\\nI\'ve been using [LastPass](https://www.lastpass.com/) for a number of years and has served me well however late last year I was reminded that there will be attempts to access your data. In December 2022, \\"an unauthorized party gained access to a third-party cloud-based storage service, which LastPass uses to store archived backups of our production data. Whilst no customer data was accessed during the August 2022 incident, some source code and technical information were stolen from our development environment and used to target another employee, obtaining credentials and keys which were used to access and decrypt some storage volumes within the cloud-based storage service.\\" [source](https://blog.lastpass.com/2022/12/notice-of-recent-security-incident/)\\n\\nI know, no customer data was stolen and even if the customer data was stolen, it is all encrypted anyway. Also, if I wasn\'t using LastPass, there was far more of chance I re-use or use weak passwords making it more likely to be exploited.\\n\\n## There is always a self-hosted alternative\\n\\nHaving a free, open-source alternative to a service I would\'ve normally paid for is awesome but this one felt really good. Enter [Bitwarden](https://bitwarden.com/). It is a fantastic, self-hosted, free alternative password manager that does pretty much everything LastPass does (all the features I\'ve been using anyway). Passwords are encrypted, it comes with an official Android and IoS app so that you can access your passwords from your phone and most importantly everything is stored locally.\\n\\n## Installing Bitwarden\\n\\nI was gearing up to write some manifests when I found a great [repo](https://github.com/guerzon/bitwarden-kubernetes) that had some great ones pre-written. Some minor tweaks and it all deployed fine. \\n\\nOne thing I deliberately didn\'t configure was a mail server (which Bitwarden needs to send out an email verification or send you a password hint if needed). I plan on revisiting this after I deploy a small mail server later.\\n\\n## Using Bitwarden\\n\\nUsing Bitwarden couldn\'t be easier either, it even comes with a handy little [guide(https://bitwarden.com/help/import-from-lastpass/)] to help you import all your passwords from LastPass\\n\\n![bitwarden](bitwarden.png)\\n\\n## Closing thoughts\\n\\nPassword managers are a must have service for everyone and whilst there are a number pretty great cloud-based password managers out there (Google and Apple included), it is quite nice knowing that I don\'t have to store my data encrypted or otherwise on a server somewhere that isn\'t one that I own and know. For that reason (and really that reason alone) I\'ll be sticking to Bitwarden.\\n\\n![password](https://media.giphy.com/media/l0G17mcoGBEabVgn6/giphy.gif)"},{"id":"certv2","metadata":{"permalink":"/blog/certv2","source":"@site/blog/2023-03-21-certs/index.md","title":"Fixing my wildcard certificates","description":"In an earlier post, I installed cert-manager to automatically manage my SSL certificates for TLS for my home-lab services. Given it was a set of services within my internal network, I was comfortable with issuing a single wildcard certificate *.local.kasj.live.","date":"2023-03-21T00:00:00.000Z","formattedDate":"March 21, 2023","tags":[{"label":"certificates","permalink":"/blog/tags/certificates"},{"label":"traffik","permalink":"/blog/tags/traffik"},{"label":"cert-manager","permalink":"/blog/tags/cert-manager"}],"readingTime":2.785,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"certv2","title":"Fixing my wildcard certificates","authors":["kas"],"tags":["certificates","traffik","cert-manager"]},"prevItem":{"title":"I switched to a self-hosted password manager and I\'m not going back","permalink":"/blog/bitwarden"},"nextItem":{"title":"Recipe and Shopping List Management with Mealie","permalink":"/blog/mealie"}},"content":"In an earlier post, I installed cert-manager to automatically manage my SSL certificates for TLS for my home-lab services. Given it was a set of services within my internal network, I was comfortable with issuing a single wildcard certificate `*.local.kasj.live`. \\n\\n## The problem\\n\\nI\'ve since realised a bit of an issue. The wildcard certificates that are issued are issued as a kubernetes `secret` resource which is specific to a namespace. This meant that in order for me to use the wildcard cert for all my services, I needed to deploy all my services into the same namespace (not ideal). \\n\\n## The research\\n\\nTurns out there were many solutions to this, many of which I don\'t really understand but I tried anyway. This included:\\n\\n* [Installing traefik again with K3s out of the box](https://major.io/2021/08/16/wildcard-letsencrypt-certificates-traefik-cloudflare/) and getting it to issue certs through Let\'s Encrypt without cert-manager. This didn\'t seem to change the behaviour I was experiencing before.\\n* [Storing the wildcard secret in a persistent volume](https://lachlan.io/blog/using-wildcard-certificates-with-traefik-and-k3s) for cert-manager to pick up as a \\"default\\" certificate when nothing else was provided. Unfortunately I couldn\'t get the persistent volume to work either\\n\\n## The solution\\n\\nI landed with a solution outline [here](https://itobey.dev/wildcard-certificates-dns-challenges-and-traefik-in-kubernetes/) and it still doesn\'t work how I want it to but its definitely a step forward. \\n\\nThe first few steps are exactly as I\'d performed in my earlier post:\\n\\n1. Install Traefik\\n2. Install Cert-Manager\\n3. Set up Let\'s Encrypt as an Issuer\\n\\nThis is where I learnt something new:\\n\\n4. Issue a wilcard certificate in the **same namespace as Traefik** in my case this was the `kube-system` namespace - I also issued myself a new one here to keep things fresh `*.home.kasj.live`\\n\\n```yaml title=\\"/home-lab/cluster-setup/cert-manager/wildcard-cert.yaml\\"\\n---\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: wildcard-home-kasj-live\\n  namespace: kube-system\\nspec:\\n  secretName: wildcard-home-kasj-live-tls\\n  issuerRef:\\n    name: letsencrypt-production\\n    kind: ClusterIssuer\\n  # commonName: \\"*.home.kasj.live\\"\\n  dnsNames:\\n    - \\"home.kasj.live\\"\\n    - \\"*.home.kasj.live\\"\\n```\\n\\nTraefik by default normally uses its own self-signed certificate for each ingress service that you define. What I needed to configure was something to tell Traefik to serve the new wildcard certifate I\'d created instead. This can be done through a kubernetes resource called **TLSStore**. \\n\\n5. Create a TLSStore resource with the name  `default`. According to the article above, it needed to be called default to be picked up by Traefik by default:\\n\\n```yaml title=\\"/home-lab/cluster-setup/cert-manager/tls-store.yaml\\"\\n---\\napiVersion: traefik.containo.us/v1alpha1\\nkind: TLSStore\\nmetadata:\\n  name: default\\n  namespace: kube-system\\nspec:\\n  defaultCertificate:\\n    secretName: wildcard-home-kasj-live-tls\\"\\n```\\n\\n6. Restart Traefik deployment so that it knows to pick up the new cert by default\\n\\n## Testing the new solution\\n\\nTo test if Traefik was issuing my new wildcard certificate by default, I created a simple nginx server and exposed it using the following manifest on `test.home.kasj.live`:\\n\\n```yaml title=\\"//home-lab/prod-apps/nginx/ingress.yaml\\"\\n---\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: nginx\\n  namespace: nginx\\n  annotations:\\n    kubernetes.io/ingress.class: traefik\\n    traefik.ingress.kubernetes.io/redirect-entry-point: https\\nspec:\\n  rules:\\n    - host: test.home.kasj.live\\n      http:\\n        paths:\\n          - backend:\\n              service:\\n                name: nginx\\n                port:\\n                  number: 80\\n            path: /\\n            pathType: Prefix\\n```\\n\\nNote: I\'ve moved away from the `IngressRoute` resource to `Ingress`\\n\\n**Success!**\\n![nginx](nginx.png)\\n![nginx2](nginx2.png)\\n\\nNow earlier, I mentioned it still wasn\'t working as I wanted it to and that\'s because there is still an issue with some services that already expose their services on SSL/HTTPS (Port 443) by default like nextcloud. Stay tuned for a future post on how I tackle that one but for now I\'m going to enjoy this win.\\n\\n![winning](winning.gif)"},{"id":"mealie","metadata":{"permalink":"/blog/mealie","source":"@site/blog/2023-02-24-mealie/index.md","title":"Recipe and Shopping List Management with Mealie","description":"During the COVID period, we start using online grocery shopping with click and collect. It has actually been saving us a bit of money as we\'re not tempted with impulse purchases while walking up and down the ailes. The downside is that the process can actually be quite cumbersome.","date":"2023-02-24T00:00:00.000Z","formattedDate":"February 24, 2023","tags":[{"label":"mealie","permalink":"/blog/tags/mealie"},{"label":"recipe","permalink":"/blog/tags/recipe"},{"label":"shopping-list","permalink":"/blog/tags/shopping-list"}],"readingTime":3.1,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"mealie","title":"Recipe and Shopping List Management with Mealie","authors":["kas"],"tags":["mealie","recipe","shopping-list"]},"prevItem":{"title":"Fixing my wildcard certificates","permalink":"/blog/certv2"},"nextItem":{"title":"Adblocker and DNS server","permalink":"/blog/dns"}},"content":"During the COVID period, we start using online grocery shopping with click and collect. It has actually been saving us a bit of money as we\'re not tempted with impulse purchases while walking up and down the ailes. The downside is that the process can actually be quite cumbersome.\\n\\nWhat we do now:\\n\\n1. Spend ages looking for meal ideas for the week\\n2. Collate a list of recipes\\n3. Collate recipe ingredients\\n4. Check the pantry and finalise shopping list\\n5. Order items in shopping list\\n\\nNow we haven\'t been necessarily doing it in that order either, we have been sporadically looking up recipe at a time and adding things to our online shopping cart over time, a lot of time. Now I don\'t think I can automate this completely but surely there was a better way to help reduce this time with a homelab app. Enter Mealie.\\n\\n## Mealie\\n\\n[Mealie](https://hay-kot.github.io/mealie/) is a self-hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application. It has some awesome features but the key ones I hope to leverage are:\\n\\n* Ability to create a custome recipe book by importing online recipes\\n* Meal planner to choose our recipes for the week\\n* Shopping list creator based on our meal plan\\n\\n## Installing Mealie\\n\\nI\'ll be using the `kompose convert` method to install Mealie. I\'m not going to cover it again but if you are interested check out my previous post as I installed **Adguard Home** with the same method.\\n\\nThe `kompose convert` command generated the following manifest files for me (I renamed them for my convenience):\\n\\n* 01-mealie-claim0-persistentvolumeclaim.yaml\\n* 02-mealie-deployment.yaml\\n* 03-mealie-service.yaml\\n\\nDeploying these files using `kubectl apply -f mealie/` deploys mealie in my cluster. Now what I need to do is expose this service to a webbrowser. If you\'ve been following my previous posts, I\'ve tried this in two ways, either giving it a network IP through Metallb or using my reverse proxy Traefik to route it through an internal domain. I\'ll be using the Traefik method today.\\n\\nSo to expose my service I create two additional files:\\n\\n`04-mealie-headers.yaml` to specify some middleware to force https:\\n\\n```yaml title=\\"04-mealie-headers.yaml\\"\\napiVersion: traefik.containo.us/v1alpha1\\nkind: Middleware\\nmetadata:\\n  name: mealie-headers\\n  namespace: mealie\\nspec:\\n  headers:\\n    browserXssFilter: true\\n    contentTypeNosniff: true\\n    forceSTSHeader: true\\n    stsIncludeSubdomains: true\\n    stsPreload: true\\n    stsSeconds: 15552000\\n    customFrameOptionsValue: SAMEORIGIN\\n    customRequestHeaders:\\n      X-Forwarded-Proto: https\\n```\\n\\n`05-mealie-ingress.yaml` to specify my routing rule so that when I navigate to `https://mealie.local.kasj.live` Traefik will route to my mealie application:\\n\\n```yaml title=\\"05-mealie-ingress.yaml\\"\\napiVersion: traefik.containo.us/v1alpha1\\nkind: IngressRoute\\nmetadata:\\n  name: mealie-ingress\\n  namespace: mealie\\n  annotations:\\n    kubernetes.io/ingress.class: traefik-external\\nspec:\\n  entryPoints:\\n    - websecure\\n  routes:\\n  - match: Host(`mealie.local.kasj.live`)\\n    kind: Rule\\n    services:\\n    - name: mealie\\n      port: 9925\\n    middlewares:\\n        - name: mealie-headers\\n  tls:\\n    secretName: local-kasj-live-tls\\n```\\n\\n## Testing mealie\\n\\nNavigate to  `mealie.local.kasj.live` and woot - we have an application!\\n\\n![mealie1](mealie1.png)\\n\\nCool feature #1 - **Recipe Import**. All I need to do is enter a recipe URL\\n\\n![mealie5](mealie5.png)\\n\\nAnd mealie imports it for me!\\n\\n![mealie6](mealie6.png)\\n\\nOnce I have a bunch of recipes imported it is time for feature #2 - **Weekly Meal Planner**. I simply just pick from my imported recipes which is pretty quick!\\n\\n![mealie2](mealie2.png)\\n\\nSweet, now that I have weekly meal plan all I need is a shopping list. Hey look feature number #3 - **Shopping List**. Mealie takes all the ingredients from the meals you\'ve selected in your weekly meal plan and throws them in a list for you.\\n\\n![mealie3](mealie3.png)\\n\\nAll we need to do now is trim the list based on what we have already, split the list between us and add to shopping cart!\\n\\n![mealie4](mealie4.png)\\n\\n\\n## Closing thoughts\\n\\nWe\'ve been using Mealie for about a week now and we\'ve cut the time spent on grocery ordering signficantly. Happy wife - thanks mealie!"},{"id":"dns","metadata":{"permalink":"/blog/dns","source":"@site/blog/2023-02-23-dns/index.md","title":"Adblocker and DNS server","description":"I\'ve always been meaning to add an adblocker to my home network and now with the additional need to have internal hostnames for my services this would be a great time to put one in. There were two great open source solutions to consider:","date":"2023-02-23T00:00:00.000Z","formattedDate":"February 23, 2023","tags":[{"label":"adguard","permalink":"/blog/tags/adguard"},{"label":"pihole","permalink":"/blog/tags/pihole"},{"label":"dns","permalink":"/blog/tags/dns"}],"readingTime":7.09,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"dns","title":"Adblocker and DNS server","authors":["kas"],"tags":["adguard","pihole","dns"]},"prevItem":{"title":"Recipe and Shopping List Management with Mealie","permalink":"/blog/mealie"},"nextItem":{"title":"Persistent volumes and NFS","permalink":"/blog/nfs"}},"content":"I\'ve always been meaning to add an adblocker to my home network and now with the additional need to have internal hostnames for my services this would be a great time to put one in. There were two great open source solutions to consider:\\n\\n* **Pihole** - Pi-hole is a general purpose network-wide ad-blocker that protects your network from ads and trackers without requiring any setup on individual devices. It is able to block ads on any network device\\n\\n* **Adguard Home** - AdGuard Home is a network-wide software for blocking ads & tracking. After you set it up, it\u2019ll cover ALL your home devices, and you don\u2019t need any client-side software for that.\\n\\n\\nHonestly I really can\'t tell the difference so I decided to install both to trial!\\n\\n## Pihole\\n\\n### Installing Pihole\\n\\nI wanted to create a bit of a file structure with all the required manifests which I can deploy at once:\\n\\n* `01-pihole-namespaces.yaml` - manifest to create a namespace\\n* `02-pihole-configs.yaml` - manifest to specify configuration values such as whitelist domains and blocklists\\n* `03-pihole-deployment.yaml` - manifest to specify deployment of pihole such as the container location\\n* `04-pihole-service.yaml` - manifest to specify by port mappings and service exposure between container and pod\\n\\n:::note\\nWorth noting here that I would normally be adding a `pihole-ingress.yaml` file here too to specify my traefik ingressRoute b resource but I won\'t be using traefik for this pihole or adguard home (as it will be a dns server)\\n:::\\n\\nI also found out that you can run `kubectl apply -f` on and entire folder which deploys all the manifests within the folder specified so in my case:\\n\\n```bash\\nkubectl apply -f pihole/\\n```\\n### Testing Pihole\\n\\nAs mentioned earlier, I didn\'t use Traefik for this service so I\'m expecting that Metallb assigned a separate IP address allocated.\\n\\n![pihole](pihole.png)\\n\\nLooks good, so I just need to navigate to `http://192.168.86.101/admin` in my webbrowser to get to the admin portal\\n\\n![pihole2](pihole2.png)\\n\\nAn voila! Happy days. I can now use this as my DNS server, define some local DNS entries and start blocking some ads!\\n\\n## Adguard Home\\n\\n### Installing Adguard Home\\n\\nI thought I\'d try installing Adguard Home slightly differently and use the Kompose tool instead. Kompose is simple, you give it a `docker-compose.yaml` and it outputs a set of kubernetes manifests for you.\\n\\nFirst things first, we need a docker-compose file so I head on over to docker hub to grab one for adguard. The docker-compose file looks like this:\\n\\n```yaml title=\\"docker-compose.yaml\\"\\nversion: \'3.3\'\\nservices:\\n    adguard:\\n        container_name: adguardhome\\n        restart: unless-stopped\\n        volumes:\\n            - \'/my/own/workdir:/opt/adguardhome/work\'\\n            - \'/my/own/confdir:/opt/adguardhome/conf\'\\n        ports:\\n            - \'53:53/tcp\'\\n            - \'53:53/udp\'\\n            - \'67:67/udp\'\\n            - \'68:68/udp\'\\n            - \'80:80/tcp\'\\n            - \'443:443/tcp\'\\n            - \'443:443/udp\'\\n            - \'3000:3000/tcp\'\\n            - \'853:853/tcp\'\\n            - \'784:784/udp\'\\n            - \'853:853/udp\'\\n            - \'8853:8853/udp\'\\n            - \'5443:5443/tcp\'\\n            - \'5443:5443/udp\'\\n        image: run\\n```\\n\\nAfter installing [kompose](https://kompose.io/), all I run `kompose convert` to give my manifest files. Kompose gives me the following manifest files:\\n\\n* `adguard-claim0-persistentvolumeclaim.yaml`\\n* `adguard-claim1-persistentvolumeclaim.yaml`\\n* `adguard-deployment.yaml`\\n* `adguard-service.yaml`\\n\\nTo see the manifest in detail, I\'ve included them in the Appendix below. \\n\\n:::note\\nI did need to make a slight change to the `adguard-service.yaml` auto generated file and that was to add the `LoadBalancer` service type. This tells Kubernetes that I needed and external IP from Metallb\\n:::\\n\\nFinally I create a namespace and run all manifests with:\\n\\n```bash\\nkubectl create namespace adguard\\nkubectl apply -f adguard/ -n adguard\\n```\\n\\n## Testing Adguard Home\\n\\nAs with PiHole, I was expecting to see the pods running and an external IP that I could navigate to with my browser:\\n\\n![adguard](adguard.png)\\n\\nSweet - looks like `192.168.86.102` was allocated.\\n\\n![adguard2](adguard2.png)\\n\\n## Closing thoughts\\n\\nBoth Pihole and Adguard Home are very similar from a feature set perspective so I haven\'t really managed to separate them as yet. If I was being super picky I\'d say that Pihole is *slightly* more customisable with blocklists and Adguard Home has a *slightly* better UI. I haven\'t decided if one it better than the other so I\'ll keep them both running for now and switch DNS Servers from time to time.\\n\\n## Appendix\\n\\n### Pihole Manifests\\n\\nFor those interested in the manifests here they are:\\n\\n#### Namespace\\n\\n```yaml title=\\"01-pihole-namespaces.yaml\\"\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: pihole\\n```\\n\\n#### Configuration\\n\\n\\n```yaml title=\\"02-pihole-configs.yaml\\"\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: custom.list\\n  namespace: pihole\\ndata:\\n  custom.list: |\\n    192.168.86.41 k3smaster\\n    192.168.86.40 k3snode01\\n    192.168.86.100 traefik.local.kasj.live\\n    192.168.86.101 pihole.local.kasj.live\\n    192.168.86.100 dash.local.kasj.live\\n    192.168.86.100 grocy.local.kasj.live\\n    192.168.86.100 kuma.local.kasj.live\\n    192.168.86.100 cloud.local.kasj.live\\n    192.168.86.100 portainer.local.kasj.live\\n    192.168.86.100 argocd.local.kasj.live\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: adlists.list\\n  namespace: pihole\\ndata:\\n  adlists.list: |\\n    https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts\\n    https://adaway.org/hosts.txt\\n    https://v.firebog.net/hosts/AdguardDNS.txt\\n    https://v.firebog.net/hosts/Admiral.txt\\n    https://raw.githubusercontent.com/anudeepND/blacklist/master/adservers.txt\\n    https://s3.amazonaws.com/lists.disconnect.me/simple_ad.txt\\n    https://v.firebog.net/hosts/Easylist.txt\\n    https://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&showintro=0&mimetype=plaintext\\n    https://raw.githubusercontent.com/FadeMind/hosts.extras/master/UncheckyAds/hosts\\n    https://raw.githubusercontent.com/bigdargon/hostsVN/master/hosts\\n    https://v.firebog.net/hosts/static/w3kbl.txt\\n---\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: whitelist.txt\\n  namespace: pihole\\ndata:\\n  whitelist.txt: |\\n    ichnaea.netflix.com\\n    nrdp.nccp.netflix.com\\n    androidtvchannels-pa.googleapis.com\\n    lcprd1.samsungcloudsolution.net\\n```\\n\\n#### Deployment\\n\\n```yaml title=\\"03-pihole-deployment.yaml\\"\\nas@lappa:~$ cat home-lab/prod-apps/pihole/03-pihole-deployment.yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  labels:\\n    app: pihole\\n  name: pihole\\n  namespace: pihole\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: pihole\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 1\\n      maxUnavailable: 0\\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      labels:\\n        app: pihole\\n    spec:\\n      containers:\\n      - env:\\n        - name: TZ\\n          value: Australia/Melbourne\\n        - name: WEBPASSWORD\\n          value:\\n        - name: DNS1\\n          value: 9.9.9.9\\n        - name: DNS2\\n          value: 1.1.1.1\\n        image: pihole/pihole:latest\\n        imagePullPolicy: IfNotPresent\\n        name: pihole\\n        ports:\\n        - name: dns-tcp\\n          containerPort: 53\\n          protocol: TCP\\n        - name: dns-udp\\n          containerPort: 53\\n          protocol: UDP\\n        - name: dhcp\\n          containerPort: 67\\n          protocol: UDP\\n        - name: web\\n          containerPort: 80\\n          protocol: TCP\\n        - name: https\\n          containerPort: 443\\n          protocol: TCP\\n        resources:\\n          requests:\\n            cpu: \\"20m\\"\\n            memory: \\"512Mi\\"\\n          limits:\\n            cpu: \\"250m\\"\\n            memory: \\"896Mi\\"\\n        readinessProbe:\\n          exec:\\n            command: [\'dig\', \'@127.0.0.1\', \'cnn.com\']\\n          timeoutSeconds: 20\\n          initialDelaySeconds: 5\\n          periodSeconds: 60\\n        livenessProbe:\\n          tcpSocket:\\n            port: dns-tcp\\n          initialDelaySeconds: 15\\n          periodSeconds: 30\\n        volumeMounts:\\n        - name: etc-pihole\\n          mountPath: /etc/pihole\\n        - name: etc-dnsmasq\\n          mountPath: /etc/dnsmasq.d\\n        - name: var-log\\n          mountPath: /var/log\\n        - name: var-log-lighttpd\\n          mountPath: /var/log/lighttpd\\n        - name: adlists\\n          mountPath: /etc/pihole/adlists.list\\n          subPath: adlists.list\\n        - name: customlist\\n          mountPath: /etc/pihole/custom.list\\n          subPath: custom.list\\n      restartPolicy: Always\\n      volumes:\\n      - name: etc-pihole\\n        emptyDir:\\n          medium: Memory\\n      - name: etc-dnsmasq\\n        emptyDir:\\n          medium: Memory\\n      - name: var-log\\n        emptyDir:\\n          medium: Memory\\n      - name: var-log-lighttpd\\n        emptyDir:\\n          medium: Memory\\n      - name: adlists\\n        configMap:\\n          name: adlists.list\\n          items:\\n            - key: adlists.list\\n              path: adlists.list\\n      - name: customlist\\n        configMap:\\n          name: custom.list\\n          items:\\n            - key: custom.list\\n              path: custom.list\\n```\\n\\n#### Service\\n\\n```yaml title=\\"04-pihole-service.yaml\\"\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: pihole-udp\\n  namespace: pihole\\n  annotations:\\n    metallb.universe.tf/allow-shared-ip: dns\\nspec:\\n  selector:\\n    app: pihole\\n  ports:\\n  - protocol: UDP\\n    port: 53\\n    name: dnsudp\\n    targetPort: 53\\n  type: LoadBalancer\\n\\n---\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: pihole-tcp\\n  namespace: pihole\\n  annotations:\\n    metallb.universe.tf/allow-shared-ip: dns\\nspec:\\n  selector:\\n    app: pihole\\n  ports:\\n  - protocol: TCP\\n    port: 53\\n    name: dnstcp\\n    targetPort: 53\\n  - protocol: TCP\\n    port: 80\\n    name: web\\n    targetPort: 80\\n  type: LoadBalancer\\n```\\n\\n### Adguard Manifests\\n\\n#### Volume claims\\n\\n```yaml title=\\"adguard-claim0-persistentvolumeclaim.yaml\\"\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  creationTimestamp: null\\n  labels:\\n    io.kompose.service: adguard-claim0\\n  name: adguard-claim0\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 100Mi\\nstatus: {}\\n```\\n\\n```yaml title=\\"adguard-claim1-persistentvolumeclaim.yaml\\"\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  creationTimestamp: null\\n  labels:\\n    io.kompose.service: adguard-claim1\\n  name: adguard-claim1\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 100Mi\\nstatus: {}\\n```\\n\\n#### Deployment\\n\\n```yaml title=\\"adguard-deployment.yaml\\"\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  annotations:\\n    kompose.cmd: kompose convert\\n    kompose.version: 1.26.0 (40646f47)\\n  creationTimestamp: null\\n  labels:\\n    io.kompose.service: adguard\\n  name: adguard\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      io.kompose.service: adguard\\n  strategy:\\n    type: Recreate\\n  template:\\n    metadata:\\n      annotations:\\n        kompose.cmd: kompose convert\\n        kompose.version: 1.26.0 (40646f47)\\n      creationTimestamp: null\\n      labels:\\n        io.kompose.service: adguard\\n    spec:\\n      containers:\\n        - image: adguard/adguardhome\\n          name: adguardhome\\n          ports:\\n            - containerPort: 53\\n            - containerPort: 53\\n              protocol: UDP\\n            - containerPort: 67\\n              protocol: UDP\\n            - containerPort: 68\\n              protocol: UDP\\n            - containerPort: 80\\n            - containerPort: 443\\n            - containerPort: 443\\n              protocol: UDP\\n            - containerPort: 3000\\n            - containerPort: 853\\n            - containerPort: 784\\n              protocol: UDP\\n            - containerPort: 853\\n              protocol: UDP\\n            - containerPort: 8853\\n              protocol: UDP\\n            - containerPort: 5443\\n            - containerPort: 5443\\n              protocol: UDP\\n          resources: {}\\n          volumeMounts:\\n            - mountPath: /opt/adguardhome/work\\n              name: adguard-claim0\\n            - mountPath: /opt/adguardhome/conf\\n              name: adguard-claim1\\n      restartPolicy: Always\\n      volumes:\\n        - name: adguard-claim0\\n          persistentVolumeClaim:\\n            claimName: adguard-claim0\\n        - name: adguard-claim1\\n          persistentVolumeClaim:\\n            claimName: adguard-claim1\\nstatus: {}\\n```\\n\\n#### Service\\n\\n```yaml title=\\"adguard-service.yaml\\"\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  annotations:\\n    kompose.cmd: kompose convert\\n    kompose.version: 1.26.0 (40646f47)\\n  creationTimestamp: null\\n  labels:\\n    io.kompose.service: adguard\\n  name: adguard\\nspec:\\n  ports:\\n    - name: \\"53\\"\\n      port: 53\\n      targetPort: 53\\n    - name: 53-udp\\n      port: 53\\n      protocol: UDP\\n      targetPort: 53\\n    - name: \\"67\\"\\n      port: 67\\n      protocol: UDP\\n      targetPort: 67\\n    - name: \\"68\\"\\n      port: 68\\n      protocol: UDP\\n      targetPort: 68\\n    - name: \\"80\\"\\n      port: 80\\n      targetPort: 80\\n    - name: \\"443\\"\\n      port: 443\\n      targetPort: 443\\n    - name: 443-udp\\n      port: 443\\n      protocol: UDP\\n      targetPort: 443\\n    - name: \\"3000\\"\\n      port: 3000\\n      targetPort: 3000\\n    - name: \\"853\\"\\n      port: 853\\n      targetPort: 853\\n    - name: \\"784\\"\\n      port: 784\\n      protocol: UDP\\n      targetPort: 784\\n    - name: 853-udp\\n      port: 853\\n      protocol: UDP\\n      targetPort: 853\\n    - name: \\"8853\\"\\n      port: 8853\\n      protocol: UDP\\n      targetPort: 8853\\n    - name: \\"5443\\"\\n      port: 5443\\n      targetPort: 5443\\n    - name: 5443-udp\\n      port: 5443\\n      protocol: UDP\\n      targetPort: 5443\\n  type: LoadBalancer\\n  selector:\\n    io.kompose.service: adguard\\n```"},{"id":"nfs","metadata":{"permalink":"/blog/nfs","source":"@site/blog/2023-02-20-nfs/index.md","title":"Persistent volumes and NFS","description":"Containers and pods are ephemeral, kubernetes provides a great advantage of being able to orchestrate the deployment, scaling, deletion of pods. But what about storage? If we use a pod\'s local filesystem for a given application and that pod is deleted, the application data disappears with it. To solve for this, we need to leverage kubernetes storage classes. Kubernetes supports a number of various storage classes ranging from public cloud storage offering to local file storage. Think the best option for me is a NFS","date":"2023-02-20T00:00:00.000Z","formattedDate":"February 20, 2023","tags":[{"label":"nfs","permalink":"/blog/tags/nfs"},{"label":"persistent-volumes","permalink":"/blog/tags/persistent-volumes"}],"readingTime":2.925,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"nfs","title":"Persistent volumes and NFS","authors":["kas"],"tags":["nfs","persistent-volumes"]},"prevItem":{"title":"Adblocker and DNS server","permalink":"/blog/dns"},"nextItem":{"title":"Certificates for HTTPS","permalink":"/blog/certs"}},"content":"Containers and pods are ephemeral, kubernetes provides a great advantage of being able to orchestrate the deployment, scaling, deletion of pods. But what about storage? If we use a pod\'s local filesystem for a given application and that pod is deleted, the application data disappears with it. To solve for this, we need to leverage kubernetes **storage classes**. Kubernetes supports a number of [various storage classes](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner) ranging from public cloud storage offering to local file storage. Think the best option for me is a [NFS](https://kubernetes.io/docs/concepts/storage/storage-classes/#nfs)\\n\\n## Installing an NFS server\\n\\nSo if I were to do this properly, I\'d be running a NAS or NFS box but since I\'ve skimped on the hardware, I\'ll be installing a NFS server on the same server as my cluster. You might be thinking *\\"mate, that\'s just the same as local storage\\"* and you would be right but I wanted to eventually switch to a separate NAS so figured I\'d just learn how to do this.\\n\\nThere are plenty of tutorials available on how to install NFS on Ubuntu but i followed this one. Here are the key commands I took away to get the job done:\\n\\nInstall the NFS server and export `/nfs` which is accessible by the Kubernetes cluster:\\n\\n```bash\\nsudo su\\napt update && apt -y upgrade\\napt install -y nfs-server\\nexit\\n\\nmkdir /nfs\\ncat << EOF >> /etc/exports\\n/nfs 192.168.86.41(rw,no_subtree_check,no_root_squash)\\nEOF\\n\\nsystemctl enable --now nfs-server\\nexportfs -ar\\n```\\n\\nIf I ever add another node to my cluster I need to ensure that a NFS client package is installed  able to connect to the NFS server but this isn\'t required as my NFS server is the same as my Kubernetes node:\\n\\n```bash\\napt install -y nfs-common\\n```\\n\\n## Persistent Volumes\\n\\nNow that I have a storage location, it is probably worth mentioning that the kubernetes rescource associated to persistent storage is **Persistent Volumes**. Like any other resource, I can provision persistent volumes declaritively to whatever storage class I specify.\\n\\nOnce a persistent volume is created, an application deployment can leverage the persistent volume using a **Persistent Volume Claim**. I could be wrong here but I think only one persistent volume claim can be applied to a persistent volume.\\n\\n## Dynamic Provisioning of Persistent Volumes\\n\\nKubernetes also provides you the ability to dynamically provision storage to applications. I found a nifty little tool that someone made called [NFS subdir external provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner) which is an automatic provisioner that uses your existing and already configured NFS server to support dynamic provisioning of Kubernetes Persistent Volumes via Persistent Volume Claims. Persistent volumes are provisioned as `${namespace}-${pvcName}-${pvName}`. To install this I run:\\n\\n```bash\\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner\\n\\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\\\\n  --create-namespace \\\\\\n  --namespace nfs-provisioner \\\\\\n  --set nfs.server=192.168.86.41 \\\\\\n  --set nfs.path=/nfs\\n```\\n\\n## Testing the provisioner\\n\\nTo test the provisioner I run:\\n\\n```bash\\nkubectl get sc\\n```\\n\\n![nfs](nfs.png)\\n\\nAnd will use following persistent volume claim manifest:\\n\\n```yaml\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: nfs-test\\n  labels:\\n    storage.k8s.io/name: nfs\\n    storage.k8s.io/part-of: kubernetes-complete-reference\\n    storage.k8s.io/created-by: ssbostan\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  storageClassName: nfs-client\\n  resources:\\n    requests:\\n      storage: 1Gi\\n```\\n\\nThings of note here:\\n\\n* `name` will vary for each volume claim - I will use the convention of `<app_name>-pvc`\\n* `labels` doesn\'t change for my needs\\n* `accessModes` doesn\'t change for my needs\\n* `storageClassName` doesn\'t change for my needs\\n* `storage` will vary for the app but worth noting that the whole specified range is provisioned (not just what you use)\\n\\nThat covers all the core cluster services I reckon, time to install some apps!"},{"id":"certs","metadata":{"permalink":"/blog/certs","source":"@site/blog/2023-02-16-letsencrypt/index.md","title":"Certificates for HTTPS","description":"Automatic Certificate Management Environment (ACME) Certificates can are usually provided through issuers. LetsEncrypt is a nonprofit Certificate Authority that provides free TLS certificates to millions of websites all around the world. This is was good enough for me!","date":"2023-02-16T00:00:00.000Z","formattedDate":"February 16, 2023","tags":[{"label":"certificates","permalink":"/blog/tags/certificates"},{"label":"letsencrypt","permalink":"/blog/tags/letsencrypt"}],"readingTime":3.575,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"certs","title":"Certificates for HTTPS","authors":["kas"],"tags":["certificates","letsencrypt"]},"prevItem":{"title":"Persistent volumes and NFS","permalink":"/blog/nfs"},"nextItem":{"title":"Certificate manager for cluster","permalink":"/blog/cert-manager"}},"content":"Automatic Certificate Management Environment (ACME) Certificates can are usually provided through issuers. LetsEncrypt is a nonprofit Certificate Authority that provides free TLS certificates to millions of websites all around the world. This is was good enough for me!\\n\\n## Adding cloudflare token to cert-manager\\n\\nFirst I needed a domain name which I purchased through CloudFlare but can be from anywhere really. You guessed it - mine is `kasj.live`. From there I needed to obtain an cloudflare token which was a personal access token to manage my DNS records in my cloudflare account. I needed this as I needed to provide it to cert-manager, which will be brokering the certificates between letsencrypt and my domain.\\n\\nProviding cert-manager my cloudflare token could be done with a simple manifest:\\n\\n```yaml title=secret-cf-token.yaml\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: cloudflare-token-secret\\n  namespace: cert-manager\\ntype: Opaque\\nstringData:\\n  cloudflare-token: <redacted>\\n```\\n\\nTo apply the manifest run:\\n\\n```bash\\nkubectl apply -f secret-cf-token.yaml\\n```\\n\\n## Adding Let\'s Encrypt as an Issuer to cert-manager\\n\\nI now need to let cert-manager know that I\'ll be using Let\'s Encrypt as my certificate issuer of choice through another manifest:\\n\\n```yaml title=\\"letsencrypt-production.yaml\\"\\napiVersion: cert-manager.io/v1\\nkind: ClusterIssuer\\nmetadata:\\n  name: letsencrypt-production\\nspec:\\n  acme:\\n    server: https://acme-v02.api.letsencrypt.org/directory\\n    email: kasunj@gmail.com\\n    privateKeySecretRef:\\n      name: letsencrypt-production\\n    solvers:\\n      - dns01:\\n          cloudflare:\\n            email: kasunj@gmail.com\\n            apiTokenSecretRef:\\n              name: cloudflare-token-secret\\n              key: cloudflare-token\\n        selector:\\n          dnsZones:\\n            - \\"kasj.live\\"\\n```\\n\\nand execute using:\\n\\n```bash\\nkubectl apply -f letsencrypt-production.yaml\\n```\\n\\n## Issuing certificates\\n\\nWith the issuer now configured, all I need to do is request for a certificate. I will be hosting all my internal applications under the subdomain `local.kasj.live` so i will request for a wildcard certicate that covers `*.local.kasj.live`\\n\\nThe certificate is issued with the following manifest:\\n\\n```yaml title=\\"local-kasj-live.yaml\\"\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: local-kasj-live\\n  namespace: default\\nspec:\\n  secretName: local-kasj-live-tls\\n  issuerRef:\\n    name: letsencrypt-production\\n    kind: ClusterIssuer\\n  commonName: \\"*.local.kasj.live\\"\\n  dnsNames:\\n  - \\"local.kasj.live\\"\\n  - \\"*.local.kasj.live\\"\\n```\\n\\nand execute using:\\n\\n```bash\\nkubectl apply -f local-kasj-live.yaml\\n```\\n\\nIssuing  and validating the certificates takes time (20 minutes minimum). To check how things are progressing run:\\n\\n```bash\\nkubectl get challenges\\n```\\n\\n:::caution\\n\\nYou\'ll notice that I use the issuer name `letsencrypt-production` - I didn\'t jump straight to this but rather used `letsencrypt-staging` first to make sure all my configuration was correct. If you jump straight to production but if it doesn\'t work for whatever reason you might be locked out by letsencrypt for a period of time.\\n\\n:::\\n\\n## Testing the issued certificate\\n\\nOnce the `kubectl get challenges` command produces nothing, that\'s when you know the process is complete. To use a certificate, you need to ensure a couple of things:\\n\\n* The certificate needs to be made available in multiple namespaces. The certificate only works if it is deployed in the same namespaces as the service you are using it for. With a bit of googling I\'ve been using the following [solution](https://github.com/mittwald/kubernetes-replicator) for this.\\n\\n* We use Traefik to specify and `ingressRoute` which essentionally provides traefik with the instructions on where to route traffic hitting the reverse proxy. We can also specify here that a certificate must be used.\\n\\nTo test above, I deployed the Traefik dashboard (with the help of their documentation and TechnoTim) with the following steps:\\n\\nCreate and deploy a middleware manifest that forces https:\\n\\n```yaml title=\\"middleware.yaml\\"\\napiVersion: traefik.containo.us/v1alpha1\\nkind: Middleware\\nmetadata:\\n  name: traefik-dashboard-basicauth\\n  namespace: traefik\\nspec:\\n  basicAuth:\\n    secret: traefik-dashboard-auth\\n```\\n\\nGenerate a credential whichi is mandatory for the dashboard:\\n\\n```bash\\n# Generate a credential / password that\u2019s base64 encoded\\nhtpasswd -nb kas <redacted> | openssl base64\\n```\\n\\nCreate and apply a manifest to deploy the dashboard. Note you need to use the output from command above for the password:\\n\\n```yaml\\n---\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: traefik-dashboard-auth\\n  namespace: traefik\\ntype: Opaque\\ndata:\\n  users: <redacted hased password which is output from above>\\n```\\n\\nFinally I create a manifest for an `ingressRoute` which will route traffic from `traefik.local.kasj.live` to my dashboard using TLS certificate I just created:\\n\\n```yaml title=\\"traefik-ingress.yaml\\"\\napiVersion: traefik.containo.us/v1alpha1\\nkind: IngressRoute\\nmetadata:\\n  name: traefik-dashboard\\n  namespace: traefik\\n  annotations:\\n    kubernetes.io/ingress.class: traefik-external\\nspec:\\n  entryPoints:\\n    - websecure\\n  routes:\\n    - match: Host(`traefik.local.kasj.live`)\\n      kind: Rule\\n      middlewares:\\n        - name: traefik-dashboard-basicauth\\n          namespace: traefik\\n      services:\\n        - name: api@internal\\n          kind: TraefikService\\n  tls:\\n    secretName: local-kasj-live-tls\\n```\\n\\n## And the results\\n\\nSo now if I navigate to `https://traefik.local.kasj.live` I can not see the traefik dashboard\\n\\n![traefik](traefik.png)\\n\\nAnd more importantly with a certificate issued from Let\'s Encrypt!\\n\\n![cert](cert.png)"},{"id":"cert-manager","metadata":{"permalink":"/blog/cert-manager","source":"@site/blog/2023-02-15-certmanager/index.md","title":"Certificate manager for cluster","description":"Certificates in K3s","date":"2023-02-15T00:00:00.000Z","formattedDate":"February 15, 2023","tags":[{"label":"cert-manager","permalink":"/blog/tags/cert-manager"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":0.855,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"cert-manager","title":"Certificate manager for cluster","authors":["kas"],"tags":["cert-manager","kubernetes"]},"prevItem":{"title":"Certificates for HTTPS","permalink":"/blog/certs"},"nextItem":{"title":"Cluster reverse proxy","permalink":"/blog/traefik"}},"content":"## Certificates in K3s\\n\\nIn my previous post I mentioned that Traefik allows me to provide SSL termination certificate handling. The thing is certificates are actually an unknown resource type in the kubernetes ecosystem like \\"pods\\" or \\"services\\". \\n\\n[cert-manager](https://cert-manager.io/) adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\\n\\n![certmanager](https://cert-manager.io/images/high-level-overview.svg)\\n\\n## Installing cert-manager\\n\\nAdd customer resource definition (CRD) using a manifest from cert-manager:\\n\\n```bash\\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\\n```\\n\\nLike with traefik, I also created a `values.yaml` file for the helm installation:\\n\\n```yaml title=\\"values.yaml\\"\\ninstallCRDs: false # Oops didn\'t realise I could do it here\\nreplicaCount: 1\\nextraArgs:\\n  - --dns01-recursive-nameservers=1.1.1.1:53,9.9.9.9:53\\n  - --dns01-recursive-nameservers-only\\npodDnsPolicy: None\\npodDnsConfig:\\n  nameservers:\\n    - \\"1.1.1.1\\"\\n    - \\"9.9.9.9\\"\\n```\\nCreate namespace, add the repo and update the repo\\n\\n```bash\\nkubectl create namespace cert-manager\\nhelm repo add jetstack https://charts.jetstack.io\\nhelm repo update\\n```\\n\\nInstall cert-manager via helm\\n\\n```bash\\nhelm install cert-manager jetstack/cert-manager --namespace cert-manager --values=values.yaml --version v1.11.0\\n```\\n\\nWith cert-manager now installed it was time get some certificates!"},{"id":"traefik","metadata":{"permalink":"/blog/traefik","source":"@site/blog/2023-02-13-traefik/index.md","title":"Cluster reverse proxy","description":"Now that I have load balancer to expose my services externally, I have a couple of options:","date":"2023-02-13T00:00:00.000Z","formattedDate":"February 13, 2023","tags":[{"label":"reverse-proxy","permalink":"/blog/tags/reverse-proxy"},{"label":"traefik","permalink":"/blog/tags/traefik"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":2.065,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"traefik","title":"Cluster reverse proxy","authors":["kas"],"tags":["reverse-proxy","traefik","kubernetes"]},"prevItem":{"title":"Certificate manager for cluster","permalink":"/blog/cert-manager"},"nextItem":{"title":"Cluster load balancer","permalink":"/blog/metallb"}},"content":"Now that I have load balancer to expose my services externally, I have a couple of options:\\n\\n* Expose every service I deploy over metallb (ie. each app gets its own IP address) or;\\n* Deploy a **reverse proxy** which intercepts and routes every incoming request to the corresponding backend services.\\n\\nFrom the title, you can tell which option I went with. I went with the reverse proxy option because\\n\\n* I don\'t know how many applications I will eventually host\\n* I also don\'t need to think about which application is associated with which IP and configure DNS routes etc\\n* It can also provide SSL termination and can be used with an ACME provider (like Let\u2019s Encrypt) for automatic certificate generation (which I\'ll cover in a future post)\\n\\n## Installing Traefik\\n\\nLike Metallb, there are heaps of reverse proxy options out there but I went with a popular option [Traefik](https://traefik.io/traefik/).\\n\\n![traefik](https://traefik.io/static/83ea42c9e8101dcf2a16f380fe3aac08/053ba/diagram.webp)\\n\\nI wanted to try installing this via helm this time. Helm allows you specify custom configuration values via a `values.yaml` file so I did that first. I know its quite long but I just tweaked the defaults:\\n\\n```yaml title=\\"values.yaml\\"\\nglobalArguments:\\n  - \\"--global.sendanonymoususage=false\\"\\n  - \\"--global.checknewversion=false\\"\\n\\nadditionalArguments:\\n  - \\"--serversTransport.insecureSkipVerify=true\\"\\n  - \\"--log.level=INFO\\"\\n\\ndeployment:\\n  enabled: true\\n  replicas: 1\\n  annotations: {}\\n  podAnnotations: {}\\n  additionalContainers: []\\n  initContainers: []\\n\\nports:\\n  web:\\n    redirectTo: websecure\\n  websecure:\\n    tls:\\n      enabled: true\\n\\ningressRoute:\\n  dashboard:\\n    enabled: false\\n\\nproviders:\\n  kubernetesCRD:\\n    enabled: true\\n    ingressClass: traefik-external\\n    allowExternalNameServices: true\\n  kubernetesIngress:\\n    enabled: true\\n    allowExternalNameServices: true\\n    publishedService:\\n      enabled: false\\n\\nrbac:\\n  enabled: true\\n\\nservice:\\n  enabled: true\\n  type: LoadBalancer\\n  annotations: {}\\n  labels: {}\\n  spec:\\n    loadBalancerIP: 192.168.86.100 # this should be an IP in the MetalLB range\\n  loadBalancerSourceRanges: []\\n  externalIPs: []\\n```\\n\\nThen I needed to execute these commands to install via helm (after installing [helm](https://helm.sh/) of course):\\n\\nAdd repo\\n```bash\\nhelm repo add traefik https://helm.traefik.io/traefik\\n```\\n\\nUpdate repo\\n```bash \\nhelm repo update\\n```\\nCreate namespace\\n```bash\\nkubectl create namespace traefik\\n```\\nFinally install using helm and our custom values file:\\n```bash\\nhelm install --namespace=traefik traefik traefik/traefik --values=values.yaml\\n```\\n\\n## Verifying installation\\n\\nFinally it was time to check if installation succeeded. \\n\\n![traefikverify](traefikverify.png)\\n\\nWhat a beautiful sight, it is all working. The main thing I was happy to see was that Metallb did the job too by assigning the IP `192.168.86.100` to the traefik service. This means I can now route all incoming request (regardless of which application) to this IP and traefik will handle all the routing. This will be done through domain names which will be covered in a later post."},{"id":"metallb","metadata":{"permalink":"/blog/metallb","source":"@site/blog/2023-02-10-metallb/index.md","title":"Cluster load balancer","description":"So now I have a cluster - woot! My first thing was to get stuck in and deploying some apps but quickly realised there were a couple of other things that needed to be considered. Say I deployed a web server to my cluster using a small nginx container. It\'s up and running but how do I access it when it is only routable within my cluster. if you look at the pods IP addresses, they are all 10.1.x.x which is not in my home network.","date":"2023-02-10T00:00:00.000Z","formattedDate":"February 10, 2023","tags":[{"label":"loadbalancer","permalink":"/blog/tags/loadbalancer"},{"label":"metallb","permalink":"/blog/tags/metallb"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":2.99,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"metallb","title":"Cluster load balancer","authors":["kas"],"tags":["loadbalancer","metallb","kubernetes"]},"prevItem":{"title":"Cluster reverse proxy","permalink":"/blog/traefik"},"nextItem":{"title":"Kubernetes cluster - K3s","permalink":"/blog/k3s"}},"content":"So now I have a cluster - woot! My first thing was to get stuck in and deploying some apps but quickly realised there were a couple of other things that needed to be considered. Say I deployed a web server to my cluster using a small nginx container. It\'s up and running but how do I access it when it is only routable within my cluster. if you look at the pods IP addresses, they are all *10.1.x.x* which is not in my home network.\\n\\nWell, the answer is I need use a load balancer to expose that web server outside the cluster so that I can see it on my home network. When working in public cloud, these load balancers are usually provided by the cloud providers but I need one for my locally hosted environment.\\n\\nAfter a bit of research, I found that [metallb](https://metallb.universe.tf/) is the best solution for this. You give it a \\"pool\\" of IP addresses within your home network to allocate to services that you want to expose and it just does it (much like a DHCP server)\\n\\n## Installing applications\\n\\nSo this is my first app that I am going to install on my cluster so it took me a little bit of reading to get to this point but here are my key takeaways of installing this and any app:\\n\\n* You can specify kubernetes a *manifest* which is basically a yaml file which allows you to declaritively specify what you want to install, where to install it from and what configurations you want and how to expose it.\\n* Most container applications are containerised with docker and often come with an associated docker-compose file. There is a nifty tool called [kompose](https://kompose.io/) which allows you to take these docker-compose files and it converts it to a kubernetes manifest for you to allow it to be deployed to your cluster - I plan on using this a lot.\\n* Another popular way of installing application is Helm. Helm is a package manager similar to apt (if you\'re familiar with Ubuntu) which allows you to easily install applications on to your cluster. All you need to do is specify the repo for the application you are wanting to install and it does the rest - I plan on using this a lot too.\\n\\n## Installing metallb\\n\\nInstalling metallb is pretty easy out of the box. You are already provided with manifest to deploy using the **kubectl apply** command:\\n\\n```bash\\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.9/config/manifests/metallb-native.yaml\\n```\\n\\nThis installs metallb into a new **namespace** called **metallb-system**. Namespaces are a kubenetes construct that basically allow you a way to organise resources within your cluster. I like to think of them as \\"folders\\" in a typical file system. So for metallb, all my resources will live in the metallb-system namespace. This allows for easy troubleshooting in the future as I know where they all live.\\n\\n\\n## Configuring metallb\\n\\nOnce installed, there were some configuration changes that needed to be made. As mentioned earlier, I needed to specify a pool of IP address for metallb to allocate out. I put this into another yaml file:\\n\\n```yaml title=\\"/home-lab/cluster-setup/metallb/metallb-ipconfig.yaml\\"\\napiVersion: metallb.io/v1beta1\\nkind: IPAddressPool\\nmetadata:\\n  name: first-pool\\n  namespace: metallb-system\\nspec:\\n  addresses:\\n  - 192.168.86.100-192.168.86.110\\n\\n---\\napiVersion: metallb.io/v1beta1\\nkind: L2Advertisement\\nmetadata:\\n  name: default\\n  namespace: metallb-system\\n```\\n\\n## Verifying installation\\n\\nAll done! Here are a few commands to verify my install:\\n\\n![metallbverify](metallbverify.png)\\n\\nBut the real test is if it will allocate an IP to a service. Let\'s test it with reverse proxy service. Stay tuned as I will cover this in my next post!"},{"id":"k3s","metadata":{"permalink":"/blog/k3s","source":"@site/blog/2023-01-25-k3s/index.md","title":"Kubernetes cluster - K3s","description":"So with my hardware set up, it was time to get my software up and running. One of the main objectives of setting up this homelab was to get familiar with kubernetes so we need to get a cluster up and running so I can do more than this:","date":"2023-01-25T00:00:00.000Z","formattedDate":"January 25, 2023","tags":[{"label":"k8s","permalink":"/blog/tags/k-8-s"},{"label":"k3s","permalink":"/blog/tags/k-3-s"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":2.695,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"k3s","title":"Kubernetes cluster - K3s","authors":["kas"],"tags":["k8s","k3s","kubernetes"]},"prevItem":{"title":"Cluster load balancer","permalink":"/blog/metallb"},"nextItem":{"title":"Gear up","permalink":"/blog/hardware"}},"content":"So with my hardware set up, it was time to get my software up and running. One of the main objectives of setting up this homelab was to get familiar with kubernetes so we need to get a cluster up and running so I can do more than this:\\n\\n![dilbert](https://pbs.twimg.com/media/EDrZEKCWwAAG_Ty.jpg)\\n\\n## Installing an OS\\n\\nI needed to get an OS installed on my NUC before anything else. There are plenty of open source options out there but I stuck with trusty [Ubuntu Server 22.04.1 LTS ](https://ubuntu.com/download/server)\\n\\n## K8s vs K3s\\n\\nBefore I got stuck into deploying my kubernetes I wanted to investigate what options I had for a homelab. It boiled down to two main ones: K8s vs K3s. Both K8s and K3s share the same source code but the key difference for me was that K3s was significantly more lightweight, can be deployed much faster and still all all the key capabilities of K8s. There are a number of \\"production grade\\" features that are excluded from K3s such has handling of complex applications and intergrations with public cloud providers which I didn\'t require.\\n\\n## Installing K3s\\n\\nInstalling K3s couldn\'t be more easier out of the box and it takes no time at all. I simply needed to ssh into my freshly install ubuntu server and execute the following command:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | sh - \\n```\\n\\nand to uninstall it is:\\n\\n```bash\\n/usr/local/bin/k3s-uninstall.sh\\n```\\n\\nI must admit I ran these commands **ALOT** because there were a number of things that were installed by default which I didn\'t need (yet). I\'m not going to get into the various customisable configuration options here but there is some pretty good [documentation](https://docs.k3s.io/installation/configuration) for it. After tweaking my configuration, I ended up with the following command to install the cluster I wanted:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\\"644\\" INSTALL_K3S_EXEC=\\"--disable traefik --disable servicelb --disable kube-proxy --disable local-storage --cluster-init --tls-san 10.43.0.1\\" sh -s -\\n```\\n\\n## Accessing my K3s cluster\\n\\nOk time to run my first kubectl command. To verify that everything was running properly I run:\\n\\n```bash\\nsudo kubectl get nodes\\n```\\n\\nwhich shows me my single node in my cluster is up and running:\\n\\n![getnodes](./getnodes.png)\\n\\nI needed to run it in sudo which I thought was annoying -I had to fix this (OCD much?). The K3s kubeconfig file is stored at a rancher location /etc/rancher/k3s. \\nI *think* this is why I needed to run kubectl in sudo. So I ran the following steps to rectify that:\\n\\nCreate .kube directory in my home directory\\n\\n```bash\\nsudo mkdir /home/kas/.kube\\n```\\n\\nCopy the kubeconfig file into the newly created directory\\n\\n```bash\\nsudo cp /etc/rancher/k3s/k3s.yaml /home/kas/.kube/config\\n```\\n\\nChange ownership of the directory so that root wasn\'t needed\\n```bash\\nsudo chown kas:kas /home/kas/.kube/config\\n```\\nLet K3s know the location of the new config file (and hopefully the last time I have to use sudo for kubectl)\\n\\n```bash\\nsudo kubectl config set-cluster default --server=https://192.168.86.41:6443 --kubeconfig /home/kas/.kube/config\\n```\\n\\nI wanted to ensure I could access my cluster from my laptop without having to SSH into my ubuntu server (k3smaster) everytime. To do this I needed copy the kubeconfig file across to my laptop using scp:\\n\\nFrom my laptop I run:\\n\\n```bash\\nscp k3smaster:/home/kas/.kube/config /home/kas/.kube/config\\n```\\n\\n...and I\'m laughing::\\n\\n![getnodes](./getnodes-lappa.png)"},{"id":"hardware","metadata":{"permalink":"/blog/hardware","source":"@site/blog/2023-01-20-hardware/index.md","title":"Gear up","description":"I\'ve normally resorted to buying raspberry pi\'s (I now have a Model 2B, 3B and 4) and don\'t have any major self-hosting requirements so I considered clustering them together. Given the current ARM architecture limitations on the older models, I decided I will bite the bullet and buy some more dedicated hardware. I also figured I could:","date":"2023-01-20T00:00:00.000Z","formattedDate":"January 20, 2023","tags":[{"label":"homelab","permalink":"/blog/tags/homelab"},{"label":"self-hosting","permalink":"/blog/tags/self-hosting"},{"label":"nuc","permalink":"/blog/tags/nuc"},{"label":"hardware","permalink":"/blog/tags/hardware"}],"readingTime":1.1,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"hardware","title":"Gear up","authors":["kas"],"tags":["homelab","self-hosting","nuc","hardware"]},"prevItem":{"title":"Kubernetes cluster - K3s","permalink":"/blog/k3s"},"nextItem":{"title":"Clean up","permalink":"/blog/cable-management"}},"content":"I\'ve normally resorted to buying raspberry pi\'s (I now have a Model 2B, 3B and 4) and don\'t have any major self-hosting requirements so I considered clustering them together. Given the current ARM architecture limitations on the older models, I decided I will bite the bullet and buy some more dedicated hardware. I also figured I could:\\n\\n* Add the raspberry pi\'s into the cluster for some more resources if need be at a later date. \\n* Use the raspberry pi\'s as an isolated sandbox for testing\\n\\nWith Raspberry Pi 4\'s currently in low stocks, I started exploring mini PCs/NUCs. They are actually pretty cool, the specs are much better, the costs aren\'t that much higher and they are built to last.\\n\\nI ended up getting [Intel NUC 11 Essential Kit Celeron N4505 (Atlas Canyon)](https://www.centrecom.com.au/intel-nuc-11-essential-kit-celeron-n4505-atlas-canyon) which was on special at CentreCom. I unboxed and plugged it in immediately only to realise that it is pretty barebones unfortunately. \\n\\n![NUC](https://cdn1.centrecom.com.au/images/upload/0135473_0.jpeg)\\n\\nThe site doesn\'t cover it but it doesn\'t come with any pre-installed HDD or RAM (that\'s why it was cheap!) so I needed to get those off Amazon. Here\'s what I bought:\\n\\n2 X [8GB DDR4 RAM](https://www.amazon.com.au/dp/B08C4Z69LN?psc=1&ref=ppx_yo2ov_dt_b_product_details)\\n  \\n  ![RAM](https://m.media-amazon.com/images/I/71exOjbZWiL._AC_SX679_.jpg)\\n\\n1 x [500GB SSD Hard Drive](https://m.media-amazon.com/images/I/418VuyafoUL._AC_SL1075_.jpg)\\n  \\n  ![HDD](https://cdn.mwave.com.au/images/400/crucial_p5_plus_500gb_nvme_m2_pcie_3d_nand_ssd_ct500p5pssd8_ac46349.jpg)\\n\\n\\nInstalling them were pretty easy - plug n play, I was then ready to rock n roll!"},{"id":"cable-management","metadata":{"permalink":"/blog/cable-management","source":"@site/blog/2023-01-17-cables/index.md","title":"Clean up","description":"So first things first, just like a chef needs to keep their bench clean, think this is the perfect time to clean up my office table. More importantly this cable situation that has been driving me INSANE","date":"2023-01-17T00:00:00.000Z","formattedDate":"January 17, 2023","tags":[{"label":"cable-management","permalink":"/blog/tags/cable-management"},{"label":"desk","permalink":"/blog/tags/desk"}],"readingTime":0.565,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"cable-management","title":"Clean up","authors":["kas"],"tags":["cable-management","desk"]},"prevItem":{"title":"Gear up","permalink":"/blog/hardware"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"So first things first, just like a chef needs to keep their bench clean, think this is the perfect time to clean up my office table. More importantly this cable situation that has been driving me **INSANE**\\n\\n![Untidy Cables](./untidy_cables.jpg)\\n\\n### Cable Management\\n\\nI\'ve had a couple of attempts at cable management but it has always been *half-arsed* and heavy on the duct tape. It was time to do this properly. I bought a couple of [cable cradles](https://www.amazon.com.au/gp/product/B09NVYV5NB/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1) from Amazon that fit nicely under the table. From there it was relativey easy to run all the cables and extension cords through. Here\'s the final result:\\n\\n![Tidy Cables 1](./tidycable_1.jpg)\\n![Tidy Cables 2](./tidycable_2.jpg)\\n![Tidy Cables 3](./tidycable_3.jpg)"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2023-01-15-welcome/index.md","title":"Welcome","description":"I\'ve been dabbling with a few self hosted services on my laptop or a raspberry pi for a while now but I now have a need to run some of these services on going. I\'ve also wanted to become more comfortable with kubernetes administration so this is also a great opportunity for me to learn something but also run a few practical services at home.","date":"2023-01-15T00:00:00.000Z","formattedDate":"January 15, 2023","tags":[{"label":"homelab","permalink":"/blog/tags/homelab"},{"label":"self-hosting","permalink":"/blog/tags/self-hosting"}],"readingTime":0.975,"hasTruncateMarker":false,"authors":[{"name":"Kas J","title":"Author","url":"https://github.com/kasjayatissa","imageURL":"https://avatars.githubusercontent.com/u/90017589?v=4","key":"kas"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["kas"],"tags":["homelab","self-hosting"]},"prevItem":{"title":"Clean up","permalink":"/blog/cable-management"}},"content":"I\'ve been dabbling with a few self hosted services on my laptop or a raspberry pi for a while now but I now have a need to run some of these services on going. I\'ve also wanted to become more comfortable with kubernetes administration so this is also a great opportunity for me to learn something but also run a few practical services at home.\\n\\nThere are some amazing resources available that I have been leaning on heavily but I find writing things down will help make some of the key concepts stick. I\'m hoping I can then transfer some of these skills to my work and future projects.\\n\\nI wanted to give a shout out to [technotim](https://www.youtube.com/@technotim) and [noted.lol](https://noted.lol/) who I\'ve drawn a lot of inspiration from for a number of my services.\\n\\nFull disclosure, I\'m not one to write blogs so this is a bit of a challenge in itself. I\'ve given myself a bit of license to be a keep things a bit rough as I write to keep myself motivated to continue to post regularly.\\n\\nIf you\'re interested feel free to read through my posts and head over to my [docs](/docs/intro)"}]}')}}]);